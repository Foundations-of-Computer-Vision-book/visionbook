

\newcommand{\xin}{\mathbf{x}_{\texttt{in}}}
\newcommand{\xout}{\mathbf{x}_{\texttt{out}}}
\newcommand{\xint}{\mathbf{x}_{\texttt{in}_t}}
\newcommand{\xoutt}{\mathbf{x}_{\texttt{out}_t}}
\newcommand{\localgrad}{\mathbf{L}}
\newcommand{\localgradx}{\mathbf{L}^{\mathbf{x}}}
\newcommand{\localgradtheta}{\mathbf{L}^{\theta}}
\newcommand{\costgrad}{\mathbf{g}}
\newcommand{\costgradxin}{\mathbf{g}^{\xin}}
\newcommand{\costgradxout}{\mathbf{g}^{\xout}}
\newcommand{\costgradin}{\mathbf{g}_{\texttt{in}}}
\newcommand{\costgradout}{\mathbf{g}_{\texttt{out}}}
\newcommand{\costgradtheta}{\mathbf{g}^{\theta}}
\newcommand{\costgradl}{\mathbf{g}_l}


\begin{algorithm}[h]
    \label{gradient_descent}
    \SetAlgoVlined
    \DontPrintSemicolon
    \caption{Backpropagation (for chain computation graphs)}
    {\bf Input:} parameter vector $\theta = \{\theta_l\}_{l=1}^L$, training datapoint $\{\mathbf{x}_0,\mathbf{y}\}$, ``chain" computation graph $f_1 \circ \ldots \circ f_L$, Loss function $\mathcal{L}: \mathbb{R}^N \rightarrow \mathbb{R}$\;
    {\bf Output:} gradient direction $\frac{\partial J}{\partial \theta} = \{\frac{\partial J}{\partial \theta_l}\}_{l=1}^L$ \;
    \;
    \textbf{Forward pass:}\;
    \For{\upshape $l=1, \dots, L$} {
        $\mathbf{x}_l = f_l(\mathbf{x}_{l-1}, \theta_l)$\;
    }
    %$J = \mathcal{L}(\mathbf{x}_L,y)$\;
    \;
    \textbf{Backward pass:}\;
    $\costgrad_L = \mathcal{L}^{\prime}(\mathbf{x}_L,y)$\;
    \For{\upshape $l=L, \dots, 1$} {
    $\localgrad_{l} = f_l^{\prime}(\mathbf{x}_{l-1},\theta_l)$\;
    $\costgrad_{l-1} = \costgrad_{l}\localgrad^{\mathbf{x}}_{l}$\;
    $\frac{\partial J}{\partial \theta_l} = \costgrad_{l}\localgrad^{\theta}_{l}$\;
    }
    \label{alg:backpropagation:backprop_for_chains}
\end{algorithm}
