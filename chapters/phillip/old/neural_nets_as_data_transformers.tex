\usetikzlibrary{arrows,arrows.spaced,decorations.markings}
\pgfplotsset{colormap/viridis}

\definecolor{gray_neuron}{rgb}{0.7,0.7,0.7}
\newcommand{\xin}{\mathbf{x}_{\texttt{in}}}
\newcommand{\xout}{\mathbf{x}_{\texttt{out}}}
\newcommand{\xini}{x_{\texttt{in}_i}}
\newcommand{\xouti}{x_{\texttt{out}_i}}
\newcommand{\xink}{x_{\texttt{in}_k}}

\definecolor{param_color}{rgb}{0.2, 0.6, 1.0}
\definecolor{data_color}{rgb}{1.0, 0.4, 0.38}

% https://tex.stackexchange.com/questions/27279/how-to-make-an-arrow-bigger-and-change-its-color-in-tikz/27287#27287
\tikzset{
  nn_edge/.style={
    decoration={markings,mark=at position 1 with {\arrow[scale=0.9]{spaced latex}}},
    postaction={decorate},
    shorten >=3.0pt
    }}

%\setcounter{chapter}{8}
\chapter{Neural nets as distribution transformers}\label{chapter:neural_nets_as_distribution_transformers}

\usetikzlibrary{arrows,arrows.spaced,decorations.markings}
\pgfplotsset{
    compat=1.3,
    rep2rep axis style/.style={
            width=1.0\textwidth,
            height=1.0\textwidth,
            axis lines=bottom,
            axis line style={stealth-stealth},
            y axis line style={draw=none},
            ymajorticks=false,
            xtick distance=0.5,
            tick style={draw=none},
            xmin=0,xmax=1,
            ymin=0,ymax=1,
        },
}

\usetikzlibrary{plotmarks}
\usetikzlibrary{arrows.meta}

\definecolor{gray_neuron}{rgb}{0.7,0.7,0.7}
\newcommand{\xin}{\mathbf{x}_{\texttt{in}}}
\newcommand{\xout}{\mathbf{x}_{\texttt{out}}}
\newcommand{\xini}{x_{\texttt{in}_i}}
\newcommand{\xouti}{x_{\texttt{out}_i}}
\newcommand{\xink}{x_{\texttt{in}_k}}
\newcommand{\pdata}{p_{\texttt{data}}}
\newcommand{\pin}{p_{\texttt{in}}}
\newcommand{\pout}{p_{\texttt{out}}}

\definecolor{param_color}{rgb}{0.2, 0.6, 1.0}
\definecolor{data_color}{rgb}{1.0, 0.4, 0.38}

% https://tex.stackexchange.com/questions/27279/how-to-make-an-arrow-bigger-and-change-its-color-in-tikz/27287#27287
\tikzset{
  nn_edge/.style={
    decoration={markings,mark=at position 1 with {\arrow[scale=0.9]{spaced latex}}},
    postaction={decorate},
    shorten >=3.0pt
    }}


\section{Introduction}
So far we have seen that deep nets are stacks of simple functions, which compose to achieve interesting mappings from inputs to outputs. This section will introduce a slightly different way of thinking about deep nets. The idea is think of each layer as a \emph{geometric transformation of a data distribution}.

\section{A different way of plotting functions}
Each layer in a deep net is a mapping from one representation of the data to another: $f: \xin \rightarrow \xout$. If $\xin$ and $\xout$ are both 1-dimensional, then we can plot the mapping as a function with $\xin$ on the x-axis and $\xout$ on the y-axis:

\begin{figure}[h]
\centerline{
\begin{tikzpicture}
    \begin{axis}[
            width=0.35\textwidth,
            height=0.35\textwidth,
            xlabel={$\xin$},
            ylabel={$\xout$},
            axis lines=center,
            xtick distance=0.5,
            ytick distance=0.5,
            tick style={draw=none},
            xmin=0,xmax=1,
            ymin=0,ymax=1
    ]
    
    \addplot +[mark=none,smooth,black] {x};
    
    \end{axis}
\end{tikzpicture}
}
\caption{The traditional way of plotting a function.}
\end{figure}

Now, we will instead consider a different way of plotting the mapping, where we simply rotate the y-axis to be horizontal rather than vertical:

\begin{figure}[h]
\centering
\begin{minipage}[t][4.5cm][c]{0.35\textwidth}
\begin{tikzpicture}
    \begin{axis}[
            width=1.0\textwidth,
            height=1.0\textwidth,
            xlabel={$\xin$},
            ylabel={$\xout$},
            axis lines=center,
            tick style={draw=none},
            xtick distance=0.5,
            ytick distance=0.5,
            xmin=0,xmax=1,
            ymin=0,ymax=1,
    ]
    
    \addplot +[mark=none,smooth,black] {x};
    
    \addplot[scatter src=explicit symbolic, only marks, mark=triangle*, mark options={rotate=90}] coordinates {
        (0.03,0.1)
        (0.03,0.2)
        (0.03,0.3)
        (0.03,0.4)
        (0.03,0.5)
        (0.03,0.6)
        (0.03,0.7)
        (0.03,0.8)
    };
    
    \addplot[scatter src=explicit symbolic, only marks, mark=*] coordinates {
        (0.1,0)
        (0.2,0)
        (0.3,0)
        (0.4,0)
        (0.5,0)
        (0.6,0)
        (0.7,0)
        (0.8,0)
    };
    
    \addplot[mark=none,dashed] coordinates {
    	(0.1,0)
    	(0.1,0.1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.2,0)
    	(0.2,0.2)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.3,0)
    	(0.3,0.3)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.4,0)
    	(0.4,0.4)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.5,0)
    	(0.5,0.5)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.6,0)
    	(0.6,0.6)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.7,0)
    	(0.7,0.7)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.8,0)
    	(0.8,0.8)
    };
    
    \addplot[mark=none,dashed] coordinates {
    	(0,0.1)
    	(0.1,0.1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0,0.2)
    	(0.2,0.2)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0,0.3)
    	(0.3,0.3)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0,0.4)
    	(0.4,0.4)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0,0.5)
    	(0.5,0.5)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0,0.6)
    	(0.6,0.6)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0,0.7)
    	(0.7,0.7)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0,0.8)
    	(0.8,0.8)
    };
    
\end{axis}
\end{tikzpicture}
\end{minipage}
\begin{minipage}[t][4.5cm][c]{0.1\textwidth}
\begin{tikzpicture}
    \draw [line width=1pt, double distance=3pt, -{Classical TikZ Rightarrow[length=3mm]}] (0,0) -- (0.7,0);
\end{tikzpicture}
\end{minipage}
\begin{minipage}[t][4.5cm][c]{0.35\textwidth}
\begin{tikzpicture}
    \begin{axis}[
            rep2rep axis style,
            xlabel={$\xin$},
    ]
    \end{axis}
    
    \begin{axis}[
            rep2rep axis style,
            xlabel={$\xout$},
            axis lines=top,
    ]
    
    \addplot[mark=none,dashed] coordinates {
    	(0.1,0)
    	(0.1,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.2,0)
    	(0.2,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.3,0)
    	(0.3,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.4,0)
    	(0.4,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.5,0)
    	(0.5,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.6,0)
    	(0.6,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.7,0)
    	(0.7,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.8,0)
    	(0.8,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.9,0)
    	(0.9,1)
    };
    
    \addplot[scatter src=explicit symbolic, only marks, mark=*] coordinates {
        (0.1,0)
        (0.2,0)
        (0.3,0)
        (0.4,0)
        (0.5,0)
        (0.6,0)
        (0.7,0)
        (0.8,0)
        (0.9,0)
    };
    
    \addplot[scatter src=explicit symbolic, only marks, mark=triangle*] coordinates {
        (0.1,0.97)
        (0.2,0.97)
        (0.3,0.97)
        (0.4,0.97)
        (0.5,0.97)
        (0.6,0.97)
        (0.7,0.97)
        (0.8,0.97)
        (0.9,0.97)
    };
    
\end{axis}
\end{tikzpicture}
\end{minipage}
\caption{An alternative way of plotting a function (right). Functions are mappings that rearrange the input space. $y=x$, shown here, means ``no rearrangement'', so the mapping is straight lines.}
\end{figure}

The depiction to the right makes it obvious that the plot $\xout = \xin$ is the identity mapping: datapoints get mapped to unchanged positions. Here are a few more mappings plotted in this way:
\begin{figure}[h]
\begin{minipage}[t][5.5cm][c]{0.45\textwidth}
\begin{flushleft}
\begin{tikzpicture}
    \begin{axis}[
            rep2rep axis style,
            width=0.78\textwidth,
            height=0.78\textwidth,
            title={$\xout = 2\xin$},
            title style={at={(0.1,0.5)},anchor=north east,draw=black,fill=white},
            xlabel={$\xin$},
            xmin=-1.2,xmax=1.2,
    ]
    \end{axis}
    
    \begin{axis}[
            rep2rep axis style,
            width=0.78\textwidth,
            height=0.78\textwidth,
            xlabel={$\xout$},
            axis lines=top,
            xmin=-1.2,xmax=1.2,
    ]
    
    \addplot[mark=none,dashed] coordinates {
    	(-0.5,0)
    	(-1.0,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(-0.25,0)
    	(-0.5,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0,0)
    	(0,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.25,0)
    	(0.5,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.5,0)
    	(1,1)
    };
    
    \addplot[scatter src=explicit symbolic, only marks, mark=*] coordinates {
        (-0.5,0)
        (-0.25,0)
        (0,0)
        (0.25,0)
        (0.5,0)
    };
    
    \addplot[scatter src=explicit symbolic, only marks, mark=triangle*] coordinates {
        (-1,0.97)
        (-0.5,0.97)
        (0,0.97)
        (0.5,0.97)
        (1,0.97)
    };
\end{axis}
\end{tikzpicture}
\end{flushleft}
\end{minipage}
\begin{minipage}[t][5.5cm][c]{0.45\textwidth}
\begin{flushright}
\begin{tikzpicture}
    \begin{axis}[
            rep2rep axis style,
            width=0.78\textwidth,
            height=0.78\textwidth,
            title={$\xout = \frac{\xin + 1}{2}$},
            title style={at={(0.1,0.5)},anchor=north east,draw=black,fill=white},
            xlabel={$\xin$},
            xmin=-1.2,xmax=1.2,
    ]
    \end{axis}
    
    \begin{axis}[
            rep2rep axis style,
            width=0.78\textwidth,
            height=0.78\textwidth,
            xlabel={$\xout$},
            axis lines=top,
            xmin=-1.2,xmax=1.2,
    ]
    
    \addplot[mark=none,dashed] coordinates {
    	(-1.0,0)
    	(0,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(-0.5,0)
    	(0.25,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0,0)
    	(0.5,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.5,0)
    	(0.75,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(1,0)
    	(1,1)
    };
    
    \addplot[scatter src=explicit symbolic, only marks, mark=*] coordinates {
        (-1,0)
        (-0.5,0)
        (0,0)
        (0.5,0)
        (1,0)
    };
    
    \addplot[scatter src=explicit symbolic, only marks, mark=triangle*] coordinates {
        (0,0.97)
        (0.25,0.97)
        (0.5,0.97)
        (0.75,0.97)
        (1,0.97)
    };
\end{axis}
\end{tikzpicture}
\end{flushright}
\end{minipage}
\begin{minipage}[t][5.5cm][c]{0.45\textwidth}
\begin{flushleft}
\begin{tikzpicture}
    \begin{axis}[
            rep2rep axis style,
            width=0.78\textwidth,
            height=0.78\textwidth,
            title={$\xout = \texttt{relu}(\xin)$},
            title style={at={(0.1,0.5)},anchor=north east,draw=black,fill=white},
            xlabel={$\xin$},
            xmin=-1.2,xmax=1.2,
    ]
    \end{axis}
    
    \begin{axis}[
            rep2rep axis style,
            width=0.78\textwidth,
            height=0.78\textwidth,
            xlabel={$\xout$},
            axis lines=top,
            xmin=-1.2,xmax=1.2,
    ]
    
    \addplot[mark=none,dashed] coordinates {
    	(-1,0)
    	(0,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(-0.5,0)
    	(0,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0,0)
    	(0,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.5,0)
    	(0.5,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(1,0)
    	(1,1)
    };
    
    \addplot[scatter src=explicit symbolic, only marks, mark=*] coordinates {
        (-1,0)
        (-0.5,0)
        (0,0)
        (0.5,0)
        (1,0)
    };
    
    \addplot[scatter src=explicit symbolic, only marks, mark=triangle*] coordinates {
        (0,0.97)
        (0.5,0.97)
        (0,0.97)
        (0.5.5,0.97)
        (1, 0.97)
    };
\end{axis}
\end{tikzpicture}
\end{flushleft}
\end{minipage}
\begin{minipage}[t][5.5cm][c]{0.45\textwidth}
\begin{flushright}
\begin{tikzpicture}
    \begin{axis}[
            rep2rep axis style,
            width=0.78\textwidth,
            height=0.78\textwidth,
            title={$\xout = \texttt{sigmoid}(\xin)$},
            title style={at={(0.1,0.5)},anchor=north east,draw=black,fill=white},
            xlabel={$\xin$},
            xmin=-5.2,xmax=5.2,
            xtick distance=2.5,
    ]
    \end{axis}
    
    \begin{axis}[
            rep2rep axis style,
            width=0.78\textwidth,
            height=0.78\textwidth,
            xlabel={$\xout$},
            axis lines=top,
            xmin=-1.2,xmax=1.2,
    ]
    
    \addplot[mark=none,dashed] coordinates {
    	(-1.0,0)
    	(0.007,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(-0.5,0)
    	(0.076,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0,0)
    	(0.5,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.5,0)
    	(0.924,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(1,0)
    	(0.993,1)
    };
    
    \addplot[scatter src=explicit symbolic, only marks, mark=*] coordinates {
        (-1,0)
        (-0.5,0)
        (0,0)
        (0.5,0)
        (1,0)
    };
    
    \addplot[scatter src=explicit symbolic, only marks, mark=triangle*] coordinates {
        (0.007,0.97)
        (0.076,0.97)
        (0.5,0.97)
        (0.924,0.97)
        (0.993,0.97)
    };
    
\end{axis}
\end{tikzpicture}
\end{flushright}
\end{minipage}
\caption{Mapping plots for several simple functions that could be neural layers.}
\end{figure}

Each of the above are layers that could be found in a deep net. Linear layers, like those in the top row above, stretch and squash the data distribution. The \texttt{relu} nonlinearity maps all negative data to 0, and applies an identity map to all non-negative data. The \texttt{sigmoid} function pulls negative data to 0 and positive data to 1.

\section{How deep nets remap a data distribution}
In this way, an incoming data distribution can be reshaped layer by layer into a desired configuration. The goal of a binary softmax classifier, for example, is to move the datapoints around until all the class 0 points end up moved to [1,0] on the output layer and all the class 1 end up moved to [0,1].

A deep net stacks these operations, like so:
\begin{figure}[h]
%\noindent\hspace{0.25\linewidth}
\centerline{
\begin{tikzpicture}
    \begin{axis}[
            rep2rep axis style,
            width=0.35\textwidth,
            height=0.4\textwidth,
            title={$\mathbf{x}^{(1)} = 2*\mathbf{x}^{(0)}$},
            title style={at={(0.1,0.25)},anchor=north east,draw=black,fill=white},
            xlabel={$\mathbf{x}^{(0)}$},
            xmin=-1.2,xmax=1.2,
    ]
    \end{axis}
    
    % \begin{axis}[
    %         rep2rep axis style,
    %         width=0.35\textwidth,
    %         height=0.25\textwidth,
    %         title={$\mathbf{x}^{(2)} = \texttt{relu}(\mathbf{x}^{(1)})$},
    %         title style={at={(0.1,1.5)},anchor=north east,draw=black,fill=white},
    %         xlabel={$\mathbf{x}^{(1)}$},
    %         axis lines=top,
    %         xmin=-1.2,xmax=1.2,
    % ]
    % \end{axis}
    
    \begin{axis}[
            rep2rep axis style,
            width=0.35\textwidth,
            height=0.4\textwidth,
            title={$\mathbf{x}^{(2)} = \texttt{relu}(\mathbf{x}^{(1)})$},
             title style={at={(0.1,0.75)},anchor=north east,draw=black,fill=white},
            xlabel={$\mathbf{x}^{(2)}$},
            axis lines=top,
            xmin=-1.2,xmax=1.2,
            ymin=0,ymax=2
    ]
    
    \addplot[mark=none] coordinates {
    	(-1.2,1)
    	(1.2,1)
    };
    
    \addplot[mark=none,dashed] coordinates {
    	(-0.5,0)
    	(-1.0,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(-0.25,0)
    	(-0.5,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0,0)
    	(0,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.25,0)
    	(0.5,1)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.5,0)
    	(1,1)
    };
    
    \addplot[scatter src=explicit symbolic, only marks, mark=*] coordinates {
        (-0.5,0)
        (-0.25,0)
        (0,0)
        (0.25,0)
        (0.5,0)
    };
    
    \addplot[scatter src=explicit symbolic, only marks, mark=triangle*] coordinates {
        (-1,0.97)
        (-0.5,0.97)
        (0,0.97)
        (0.5,0.97)
        (1,0.97)
    };
    
    \addplot[mark=none,dashed] coordinates {
    	(-1,1)
    	(0,2)
    };
    \addplot[mark=none,dashed] coordinates {
    	(-0.5,1)
    	(0,2)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0,1)
    	(0,2)
    };
    \addplot[mark=none,dashed] coordinates {
    	(0.5,1)
    	(0.5,2)
    };
    \addplot[mark=none,dashed] coordinates {
    	(1,1)
    	(1,2)
    };
    
    \addplot[scatter src=explicit symbolic, only marks, mark=triangle*] coordinates {
        (0,1.97)
        (0,1.97)
        (0,1.97)
        (0.5,1.97)
        (1,1.97)
    };
    
\end{axis}

\end{tikzpicture}
}
\caption{Mapping plot for a \texttt{linear}-\texttt{relu} stack.}
\end{figure}


The plots above show how a uniform grid of datapoints get mapped from layer to layer in a deep net. We can also use this plotting style to show how a non-uniform distribution of incoming datapoints gets transformed. This is the setting in which deep nets actually operate, and sometimes the real action of the network looks very different when viewed this way. We can think of a deep net as transforming an input data distribution, $\pdata$, into an output data distribution, $\pout$. Each layer of activations in a network is a different representation or {\bf embedding} of the data, and we can consider the distribution of activations on some layer $\ell$ to be $p_{\ell}$. Then, layer by layer, a deep net transforms $\pdata$ into $p_{\texttt{1}}$ into $p_{\texttt{2}}$, and so on until finally transforming the data to the distribution $\pout$. Most loss functions can also be interpreted from this angle: they penalize the divergence, in one form or another, between the output distribution $\pout$ and a target distribution $p_{\texttt{target}}$.

A nice property of the above way of plotting is that it also extends to visualizing 2D-to-2D mappings (something that conventional x-axis/y-axis plotting is not well equipped to do). Real deep nets perform ND-to-ND mappings, but already 2D-to-2D visualizations can give a lot of insight into the general case. In Figure \ref{fig:neural_nets_as_data_transformations:2D_mapping_diagrams}, we show how three common neural net layers may act to transform a Gaussian blob of data centered at the origin.
\begin{figure}[h]
\centerline{
\begin{minipage}{0.36\textwidth}
\begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.8\linewidth]{figures/neural_nets/linear_layer_mapviz.pdf}};
    \draw (-1, -1.35) node {$\xin$};
    \draw (-1, 1.4) node {$\xout$};
    \draw (0, 2) node {\texttt{linear}};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.28\textwidth}
\begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.8\linewidth]{figures/neural_nets/relu_layer_mapviz.pdf}};
    \draw (-1, -1.35) node {$\xin$};
    \draw (-1, 1.4) node {$\xout$};
    \draw (0, 2) node {\texttt{relu}};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.28\textwidth}
\begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.8\linewidth]{figures/neural_nets/L2norm_layer_mapviz.pdf}};
    \draw (-1, -1.35) node {$\xin$};
    \draw (-1, 1.4) node {$\xout$};
    \draw (0, 2) node {\texttt{L2-norm}};
\end{tikzpicture}
\end{minipage}
% \begin{minipage}{0.33\textwidth}
% \begin{flushright}
% \begin{tikzpicture}
%     \draw (0, 0) node[inner sep=0] {\includegraphics[width=1.0\linewidth]{figures/neural_nets/softmax_layer_mapviz.pdf}};
% \end{tikzpicture}
% \end{flushright}
% \end{minipage}
}
\caption{2D mapping diagrams for several neural layers.}
\label{fig:neural_nets_as_data_transformations:2D_mapping_diagrams}
\end{figure}


%Here is a linear layer $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$:
% Here are the linear and relu layers plotted this way, alongside other ways of representing these mappings:
% \begin{figure}[h]
%     \centerline{
%     \includegraphics[width=1.0\linewidth]{./figures/neural_nets/layer_cards_linear_relu.pdf}}
%     \caption{A summary of the main two layers we have learned about so far.}
%     \label{fig:neural_nets:layer_cards_linear_relu}
% \end{figure}

One interesting thing to notice here is that the \texttt{relu} layer maps many points to the axes of the positive quadrant. In general, with \texttt{relu}-nets, a lot of data density will build up along these axes, because any point \textit{not} in the positive quadrant snaps to an axis. This effect becomes exaggerated in real networks with high-dimensional embeddings. In particular, for a width-$N$ layer, the region that is strictly positive occupies only $\frac{1}{2^N}$ proportion of the embedding space, so almost the entire space gets mapped to the axes after a \texttt{relu} layer. The geometry of high-dimensional neural representations may become very sparse because of this, where most of the volume of representational space is not occupied by any datapoints.
%This is because relu snaps all negative coordinates to zero, so any point in the negative subspace will end up mapping to an ``edge" of the positive quadrant.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.36\linewidth]{./figures/neural_nets_as_data_transformations/2D_linear_layer.png}
%     \label{fig:neural_nets_as_data_transformations:2D_linear_layer}
% \end{figure}

%The data can be thought of as itself a representation. The target output, e.g., class labels, is another representation. Then a classifier network, layer by layer, maps a representations that matches the raw data to a representation that matches the labels. The sequence of representations for an input datapoint $\xin^i$ is $\xin^i \rightarrow \mathbf{h}^{(1)i} \rightarrow \mathbf{h}^{(2)i} \rightarrow \cdots \rightarrow \xout^i$.

%One way to characterize this sequence of representations is by looking at how each subsequent layer represents an entire \emph{dataset}. 


\section{Binary classifier example}
Consider an MLP that performs binary classification formulated as 2-way softmax regression. The input datapoints are each in $\mathbb{R}^2$ and the target outputs are in $\vartriangle^1$ (the 1-simplex),\marginnote{The {\bf N-simplex}, $\vartriangle^N$, is the set of all $N+1$ dimensional vectors whose elements sum to 1. $N+1$ dim one-hot codes live on the vertices of $\vartriangle^{N}$.}[-1.0cm] and there is one 2D hidden layer (with pre- and post-activations). This network can be drawn as follows:

\begin{figure}[h]
\centerline{

\noindent\hspace{0.05\linewidth}
\begin{minipage}{.45\linewidth}

\begin{tikzpicture}[>=spaced latex]
\draw [thick] (0,-0.4) circle [radius=0.3] node {$x_2$};
\draw [thick] (0,0.4) circle [radius=0.3] node {$x_1$};
\draw [thick] [nn_edge] (0.3,-0.4) -- (1.0,-0.4);
\draw [thick] [nn_edge] (0.3,0.4) -- (1.0,0.4);
\draw [thick] [nn_edge] (0.3,-0.4) -- (1.0,0.4);
\draw [thick] [nn_edge] (0.3,0.4) -- (1.0,-0.4);
\draw [thick, fill=gray_neuron] (1.3,-0.4) circle [radius=0.3] node {$z_{1_1}$};
\draw [thick, fill=gray_neuron] (1.3,0.4) circle [radius=0.3] node {$z_{1_2}$};
\draw [thick,dotted] (0.5,-0.9)  .. controls (0.6,-0.75) .. (0.6,-0.6);
\draw (0.6,-1.2) node {$\mathbf{W}_1$};

\draw [thick] [nn_edge] (1.6,-0.4) -- (2.3,-0.4);
\draw [thick] [nn_edge] (1.6,0.4) -- (2.3,0.4);
\draw [thick, fill=gray_neuron] (2.6,-0.4) circle [radius=0.3] node {$h_{1_1}$};
\draw [thick, fill=gray_neuron] (2.6,0.4) circle [radius=0.3] node {$h_{1_2}$};
\draw [thick,dotted] (3.2,-0.9)  .. controls (3.3,-0.75) .. (3.3,-0.6);
\draw (3.3,-1.2) node {$\mathbf{W}_2$};

\draw [thick] [nn_edge] (2.9,-0.4) -- (3.6,0.4);
\draw [thick] [nn_edge] (2.9,0.4) -- (3.6,-0.4);
\draw [thick] [nn_edge] (2.9,-0.4) -- (3.6,-0.4);
\draw [thick] [nn_edge] (2.9,0.4) -- (3.6,0.4);
\draw [thick, fill=gray_neuron] (3.9,-0.4) circle [radius=0.3] node {$z_{2_1}$};
\draw [thick, fill=gray_neuron] (3.9,0.4) circle [radius=0.3] node {$z_{2_2}$};
\draw [thick] [nn_edge] (4.2,-0.4) -- (4.9,-0.4);
\draw [thick] [nn_edge] (4.2,0.4) -- (4.9,0.4);
\draw [thick] (5.2,-0.4) circle [radius=0.3] node {$y_1$};
\draw [thick] (5.2,0.4) circle [radius=0.3] node {$y_2$};
\end{tikzpicture}
\label{fig:neural_nets:simple_MLP_network2}
\end{minipage}
}
\caption{A two-layer MLP with two outputs, suitable for performing binary softmax regression.}
\vspace{-0.5cm}
\end{figure}

Or expressed in math as:
\begin{align}
    \mathbf{z}_1 &= \mathbf{W}_1\mathbf{x} + \mathbf{b}_1 &\triangleleft \quad \texttt{linear}\\
    \mathbf{h}_1 &= \texttt{relu}(\mathbf{z}_1) &\triangleleft \quad \texttt{relu}\\
    \mathbf{z}_2 &= \mathbf{W}_2\mathbf{h}_1 + \mathbf{b}_2 &\triangleleft \quad \texttt{linear}\\
    \mathbf{y} &= \texttt{softmax}(\mathbf{z}_2) &\triangleleft \quad \text{\texttt{softmax}}
\end{align}

% Or in Pytorch-like pseudocode as:
% \begin{figure}[h]
% %\centerline{
% \begin{minipage}{0.5\linewidth}
% \begin{minted}[
% fontsize=\fontsize{8.5}{9},
% frame=single,
% framesep=2.5pt,
% baselinestretch=1.05,
% ]{python}
% # W1, b1, W2, b2 : parameters of the net
% # X : dataset to run through net

% # first define parameterized layers
% fc1 = nn.linear(W1, b1)
% fc2 = nn.linear(W2, b2)

% # then run data through network
% for x in X:
%     z1 = fc1(x)
%     h1 = nn.relu(z1)
%     z2 = fc2(h1)
%     y = nn.softmax(z2)
% \end{minted}
% \end{minipage}
% \cation{Pytorch-like pseudocode for the MLP in Figure \ref{fig:neural_nets:simple_MLP_network2}.}
% %}
% \end{figure}

Now we wish to train this net to classify between two Gaussian clouds of data, the red cloud and the blue cloud here:
\begin{figure}[h]
    \centerline{
    \includegraphics[width=0.28\linewidth]{./figures/neural_nets/binary_classifier_example_training_data.pdf}
    \label{fig:intro_to_learning:neural_nets/binary_classifier_example_training_data}
    }
    \caption{A dataset with two classes, shown in red and blue.}
    \label{fig:neural_nets:two_class_dataset}
\end{figure}

We can visualize how the net transforms the training dataset, layer by layer, at three {\bf checkpoints} over the course of training:\marginnote{A checkpoint is a record of the parameters at some iteration of training, i.e. if iterates of the parameter vector are $\theta^0, \theta^1, \ldots, \theta_T$, while training for $T$ steps of learning, then any $\theta^k$ can be recorded as a checkpoint.}[-0.0cm]
\begin{figure}[h]
    \centerline{
    \includegraphics[width=1.0\linewidth]{./figures/neural_nets/binary_classifier_example_training_iters.pdf}
    }
    \caption{Training the MLP from Figure \ref{fig:neural_nets:simple_MLP_network2} on the dataset from Figure \ref{fig:neural_nets:two_class_dataset}.}
    \label{fig:intro_to_learning:neural_nets/binary_classifier_example_training_iters}
\end{figure}

Now consider a harder classification problem. We will visualize a deeper net learning to classify between two distributions, the first circular in shape (red points) and the second forming a semi-circle around the first (blue points). In Figure \ref{fig:intro_to_learning:neural_nets/nn_training_viz}, we plot this as a 3D visualization of $\mathbb{R}^2 \rightarrow \mathbb{R}^2$ mappings. Each dotted line connects the representation of a datapoint at one layer to its representation at the next. The gray lines show, for each layer, how a square region around the origin gets stretched, rotated, or squished to map to a transformed region on the next layer.\marginnote{``Stretched, rotated, or squished'' in one way to think of it, but we can be more precise. The linear layers perform an affine transformation of the space, while the relus project all negative values to the boundaries of the cone whose dimensions are strictly positive.}
\begin{figure}[h]
    \centerline{
    \includegraphics[width=1.0\linewidth]{./figures/neural_nets/nn_training_viz2.pdf}
    }
    \caption{How a deep net remaps input data layer by layer. The target output is to move all the red points to $(1,0)$ and all the blue points to $(0,1)$ (one-hot codes for the two classes). As training progresses the network gradually achieves this separation.}
    \label{fig:intro_to_learning:neural_nets/nn_training_viz}
\end{figure}

Layer by layer, over the course of training, the net learns to disentangle these two classes and pull the points toward vertices of the 1-simplex, i.e. to correctly classify the points!


%Each layer of a deep net performs a mapping that moves the datapoints around.

\section{Visualizations beyond 2D}
%When the represntation $\mathbf{h}^{(1)i}$ were a two-dimensional vector, then we could visualize this representation for all datapoints $i$ in a dataset as a 2D scatter plot. If the next layer $\mathbf{h}^{(2)i}$ were also two-dimensional, then it would be a different scatter plot, with each datapoint shifted to a new position. Typically, each hidden layer is a high-dimensional vector, so it's non-trivial to visualize datapoints in this space.
What if our representations are high-dimensional? The plots above only can visualize 1D and 2D data distributions. Deep representations are typically much higher-dimensional than this, and to visualize them, we need to apply tools from {\bf dimensionality reduction}. These tools project the high-dimensional data to a lower dimensionality, e.g. 2D, which can be visualized. A common objective is to perform the projection such that the distance between two datapoints in the 2D projection is roughly proportional to their actual distance in the high-dimensional space. The below plot, reproduced from \cite{decaf}, uses a dimensionality reduction technique called t-SNE~\cite{tsne} to visualize how different layers of a deep net represent a dataset of images of different semantic classes, where each color represents a different semantic class:
\begin{figure}[h]
    \centerline{
    \includegraphics[width=0.6\linewidth]{./figures/neural_nets/decaf_tsne.pdf}
    }
    \caption{How a deep image classifier remaps input pixels into a ``disentangled" representation where semantic classes are separated. t-SNE visualization taken from \cite{decaf}. \reviewcomment{Replace fish photo, get permission to use tsne image.}}
    \label{fig:neural_nets:decaf_tsne}
\end{figure}

Notice that on the first layer, semantic classes are not well separated but by layer 6 the representation has {\bf disentangled} the semantic classes so that each class occupies a different part of representational space. This is expected because layer 6 is near the output of the network, and the output is being trained to be a one-hot representation of semantics -- i.e. a completely disentangled representation.

\section{Concluding remarks}
Layer by layer, deep nets transform data from its raw format to ever more abstracted and useful representations. It can be useful to think about this process as a set of geometric transformations of a data distribution, or a kind of ``disentangling" where initially messy data gets reorganized so that different data classes are cleanly separated.