
%\setcounter{chapter}{24}
\chapter{Recurrent Neural Nets}
\label{chapter:recurrent_neural_nets}

\section{Introduction}
RNNs are a neural net architecture for modeling sequencesâ€”they perform \textit{sequential} computation (rather than parallel computation). 

So far we have seen {\bf feedforward} neural net architectures. These architectures can be represented by directed acyclical computation graphs. \index{Recurrent neural network}{\bf Recurrent neural networks} ({\bf RNNs}), are neural nets with feedback connections.\marginnote{A feedback connection defines a \textbf{recurrence} in the computational graph.} Their computation graphs have cycles. That means that the outputs of one layer can send a message back to earlier in the computational pipeline. To make this well-defined, RNNs introduce the notion of \textbf{timestep}. One timestep of running an RNN corresponds to one functional call to each layer (or computation node) in the RNN, mapping that layer's inputs to its outputs.

RNNs are all about adaptation. They solve the following problem: What if we want our computations in the past to influence our computations in the future? If we have a stream of data, a feedforward net has no way to adapting its behavior based on what outputs it previously produced.

To motivate RNNs, we will first consider a deficiency of convolutional neural nets (CNNs). Consider that we are processing a temporal signal with a CNN, like so:

\begin{figure}[h]
\centerline{
\begin{tikzpicture}%[>=spaced latex]
%
\def\Nnodes{10}
\def\layerheight{1.5}
\def\neuronrad{0.2}
\def\neuronstep{0.65}
\foreach \x in {0,...,\Nnodes} {
    \draw (\neuronstep*\x,0) circle [radius=\neuronrad];
    \draw (\neuronstep*\x,\layerheight) circle [radius=\neuronrad];}
%
\draw [thick] [nn_edge] (0,0.2) -- (\neuronstep,\layerheight-\neuronrad);
\draw [thick] [nn_edge] (\neuronstep,\neuronrad) -- (\neuronstep,\layerheight-\neuronrad);
\draw [thick] [nn_edge] (\neuronstep*2,\neuronrad) -- (\neuronstep,\layerheight-\neuronrad);
\node[rectangle,draw,fill=white] (r) at (\neuronstep,0.75) {$\mathbf{w}$};
%
\node at (\neuronstep*3,0.75) {$\ldots$};
\node at (\neuronstep*8,0.75) {$\ldots$};
%
\draw [thick] [nn_edge] (\neuronstep*5,\neuronrad) -- (\neuronstep*6,\layerheight-\neuronrad);
\draw [thick] [nn_edge] (\neuronstep*6,\neuronrad) -- (\neuronstep*6,\layerheight-\neuronrad);
\draw [thick] [nn_edge] (\neuronstep*7,\neuronrad) -- (\neuronstep*6,\layerheight-\neuronrad);
\node[rectangle,draw,fill=white] (r) at (\neuronstep*6,0.75) {$\mathbf{w}$};
%
\node[inner sep=0pt] at (\neuronstep,-1.3)
    {\includegraphics[width=0.09\textwidth]{./figures/recurrent_neural_nets/mo1.jpg}};
\node[inner sep=0pt] at (6*\neuronstep,-1.3)
    {\includegraphics[width=0.09\textwidth]{./figures/recurrent_neural_nets/mo2.jpg}};
%
\draw [] [times_arrow] (-\neuronrad,-0.4) -- (\neuronstep*\Nnodes+\neuronrad-0.7,-0.4);
\draw (\neuronstep*\Nnodes+\neuronrad-0.3,-0.4) node {\small{time}};
\end{tikzpicture}
}
\caption{A CNN processing a video, with two example frames shown.}
\end{figure}

The filter $\mathbf{w}$ slides over the input signal and produces outputs. In this example, imagine the input is a video of your cat, named Mo. The output produced for the first set of frames is entirely independent from the output produced for a later frame, as long as the receptive fields do not overlap. This independence can cause problems. For instance, maybe it got dark outside as the video progressed, and a clear image of Mo became hard to discern as the light dimmed. Then the convolutional response, and the net's prediction, for later in the video cannot reference the earlier clear image, and will be limited to making the best inference it can from the dim image.% may be of low quality, and the net now predicts this is your neighbor's cat, Moxie!

Wouldn't it be nice if we could inform the CNN's later predictions that earlier in the day we had a brighter view and this was clearly Mo? With a CNN, the only way to do this is to increase the size of the convolutional filters so that the receptive fields can see sufficiently far back in time. But what if we last saw Mo a year ago? It would be infeasible to extend our receptive fields a year back. How can we humans solve this problem and recognize Mo after a year of not seeing her?

The answer is memory. The recurrent feedback loops in an RNN are a kind of memory. They propagate information from timestep $t$ to timestep $t+1$. In other words, they remember information about timestep $t$ when processing new information at timestep $t+1$. Thus, by induction, an RNN can potentially remember information over arbitrarily long time intervals. In fact, RNNs are Turing complete: you can think of them as a little computer. They can do anything a computer can do.

Here's how RNNs arrange their connections to perform this propagation:

\begin{figure}[h]
\centerline{
\begin{tikzpicture}%[>=spaced latex]
%
\def\Nnodes{10}
\def\Nnodesminusone{9}
\def\layerheight{1.5}
\def\neuronrad{0.2}
\def\neuronstep{0.65}
\foreach \x in {0,...,\Nnodes} {
    \draw (\neuronstep*\x,0) circle [radius=\neuronrad];
    \draw (\neuronstep*\x,\layerheight) circle [radius=\neuronrad];
    \draw (\neuronstep*\x,\layerheight*1.5) circle [radius=\neuronrad];
    \draw [thick] [nn_edge] (\neuronstep*\x,\neuronrad+\layerheight) -- (\neuronstep*\x,\layerheight*1.5-\neuronrad);
}
\foreach \x in {0,...,\Nnodesminusone} {
    \draw [thick] [nn_edge] (\neuronstep*\x+\neuronrad,\layerheight) -- (\neuronstep*\x+\neuronstep-\neuronrad,\layerheight);
}
%
\draw [thick] [nn_edge] (0,0.2) -- (\neuronstep,\layerheight-\neuronrad);
\draw [thick] [nn_edge] (\neuronstep,\neuronrad) -- (\neuronstep,\layerheight-\neuronrad);
\draw [thick] [nn_edge] (\neuronstep*2,\neuronrad) -- (\neuronstep,\layerheight-\neuronrad);
\node[rectangle,draw,fill=white] (r) at (\neuronstep,0.75) {$\mathbf{w}$};
%
\node at (\neuronstep*3,0.75) {$\ldots$};
\node at (\neuronstep*8,0.75) {$\ldots$};
%
\draw [thick] [nn_edge] (\neuronstep*5,\neuronrad) -- (\neuronstep*6,\layerheight-\neuronrad);
\draw [thick] [nn_edge] (\neuronstep*6,\neuronrad) -- (\neuronstep*6,\layerheight-\neuronrad);
\draw [thick] [nn_edge] (\neuronstep*7,\neuronrad) -- (\neuronstep*6,\layerheight-\neuronrad);
\node[rectangle,draw,fill=white] (r) at (\neuronstep*6,0.75) {$\mathbf{w}$};
%
\draw [] [times_arrow] (-\neuronrad,-0.4) -- (\neuronstep*\Nnodes+\neuronrad-0.7,-0.4);
\draw (\neuronstep*\Nnodes+\neuronrad-0.3,-0.4) node {\small{time}};
%
\draw (-0.9,0) node {\small{input}};
\draw (-0.9,\layerheight) node {\small{hidden}};
\draw (-0.9,\layerheight*1.5) node {\small{output}};
\end{tikzpicture}
}
\caption{Basic RNN with one hidden layer.}
\end{figure}

The key idea of an RNN is that there are lateral connections, between hidden units, through time. The inputs, outputs, and hidden units may be multidimensional tensors; to denote this we will use \tikz{\def\neuronrad{0.2} \draw (0-\neuronrad,0-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);}, as \tikz{\def\neuronrad{0.2} \draw (0,0) circle [radius=\neuronrad];} was reserved for denoting a single (scalar) neuron.
Here is a simple 1-layer RNN:
\begin{figure}[h]
\centerline{
\begin{tikzpicture}%[>=spaced latex]
%
\def\Nnodes{10}
\def\Nnodesminusone{9}
\def\layerheight{0.75}
\def\neuronrad{0.2}
\def\neuronstep{0.65}
\foreach \x in {0,...,\Nnodes} {
    \draw (\neuronstep*\x-\neuronrad,0-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);
    \draw (\neuronstep*\x-\neuronrad,\layerheight-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);
    \draw (\neuronstep*\x-\neuronrad,\layerheight*2-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);
    \draw [thick] [nn_edge] (\neuronstep*\x,\neuronrad) -- (\neuronstep*\x,\layerheight-\neuronrad);
    \draw [thick] [nn_edge] (\neuronstep*\x,\neuronrad+\layerheight) -- (\neuronstep*\x,\layerheight*2-\neuronrad);
}
\foreach \x in {0,...,\Nnodesminusone} {
    \draw [thick] [nn_edge] (\neuronstep*\x+\neuronrad,\layerheight) -- (\neuronstep*\x+\neuronstep-\neuronrad,\layerheight);
}
%
\draw [] [times_arrow] (-\neuronrad,-0.4) -- (\neuronstep*\Nnodes+\neuronrad-0.7,-0.4);
\draw (\neuronstep*\Nnodes+\neuronrad-0.3,-0.4) node {\small{time}};
%
\draw (-0.9,0) node {$\xin$};
\draw (-0.9,\layerheight) node {$\mathbf{h}$};
\draw (-0.9,\layerheight*2) node {$\xout$};
%
\draw [dotted,thick] (\neuronstep*\Nnodes+1 , 0) -- (\neuronstep*\Nnodes+1 , \layerheight*2 + \neuronrad); 
%
\draw (\neuronstep*\Nnodes-\neuronrad+2,0-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);
\draw (\neuronstep*\Nnodes-\neuronrad+2,\layerheight-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);
\draw (\neuronstep*\Nnodes-\neuronrad+2,\layerheight*2-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);
\draw [thick] [nn_edge] (\neuronstep*\Nnodes+2,\neuronrad) -- (\neuronstep*\Nnodes+2,\layerheight-\neuronrad);
\draw [thick] [nn_edge] (\neuronstep*\Nnodes+2,\neuronrad+\layerheight) -- (\neuronstep*\Nnodes+2,\layerheight*2-\neuronrad);
%\draw [thick] [nn_edge] (\neuronstep*\Nnodes+2+0.14,\layerheight-0.14) .. controls (\neuronstep*\Nnodes+2+0.55,\layerheight-0.28) and (\neuronstep*\Nnodes+2+0.55,\layerheight+0.28) ..  (\neuronstep*\Nnodes+2+0.14,\layerheight+0.14);
\draw [thick] [nn_edge] (\neuronstep*\Nnodes+2+0.2,\layerheight-0.1) .. controls (\neuronstep*\Nnodes+2+0.65,\layerheight-0.32) and (\neuronstep*\Nnodes+2+0.65,\layerheight+0.32) ..  (\neuronstep*\Nnodes+2+0.2,\layerheight+0.1);
\end{tikzpicture}
}
\caption{(left) RNN with multidimensional inputs, hidden states, and outputs. (right) the rolled-up version of this RNN, which can be unrolled for an unbounded number of steps.}
\end{figure}

To the left, is the {\bf unrolled computation graph}. To the right, is the computation graph rolled up, with a feedback connection. Typically we will work with unrolled graphs because they are directed acyclic graphs (DAGs), and therefore all the tools we have developed for DAGs, including backpropagation, will extend naturally to them. However, note that RNNs can run for unbounded time, and process signals that have unbounded temporal extent, while a DAG can only represent a finite graph. The DAG depicted above is therefore a \textit{truncated} version of the RNN's theoretical computation graph.

\section{Recurrent Layer}
A \index{Recurrent layer}{\bf recurrent layer} is defined by the following equations:
\begin{align}
    \mathbf{h}_t &= f(\mathbf{h}_{t-1}, \xint) &\triangleleft \quad\quad \text{update state based on input and previous state}\\
    \xoutt &= g(\mathbf{h}_t) &\triangleleft \quad\quad \text{produce output based on state}
\end{align}
This is a layer that includes a state variable, $\mathbf{h}$. The operation of the layer depends on its state. The layer also updates its state every time it is called, therefore the state is a kind of memory.
\marginnote{The idea of a \textit{time} in an RNN does not necessarily mean time as it is measured on a clock. Time just refers to \textit{sequential computation}; the timestep $t$ is an index into the sequence. The sequential computation could progress over a spatial dimension (processing an image pixel by pixel) or over the temporal dimension (processing a video frame by frame), or even over more abstracted computational modules.}[-2cm]

The $f$ and $g$ can be arbitrary functions, and we can imagine a wide variety of recurrent layers defined by different choices for the form of $f$ and $g$.

One common kind of recurrent layer is the \index{Recurrent layer!Simple recurrent layer}{\bf Simple Recurrent Layer} ({\bf SRN}), which was defined by Elman in ``Finding Structure in Time''~\cite{elman1990finding}. For this layer, $f$ is a linear layer followed by a pointwise nonlinearity, and $g$ is another linear layer. The full computation is defined by the following equations:
\begin{align}
    \mathbf{h}_t &= \sigma_1(\mathbf{W}\mathbf{h}_{t-1} + \mathbf{U}\xint + \mathbf{b})\\
    \xoutt &= \sigma_2(\mathbf{V}\mathbf{h}_t + \mathbf{c})
\end{align}
where $\sigma_1$ and $\sigma_2$ are pointwise nonlinearities. In SRNs, tanh is the common choice for the nonlinearity, but relus and other choices may also be used.


\section{Backpropagation through Time}\label{sec:recurrent_neural_nets:bptt}
Backpropagation is only defined for DAGs and RNNs are not DAGs. To apply backpropagation to RNNs, we unroll the net for $T$ timesteps, which creates a DAG representation of the truncated RNN, and then do backpropagation through that DAG. This is known as \index{Backpropagation through time}{\bf backpropagation through time} ({\bf BPTT}). 

This approach yields exact gradients of the loss with respect to the parameters when $T$ is equal to the total number of timesteps the RNN was run for. Otherwise, BPTT is an approximation that truncates the true computation graph. Essentially, BPTT ignores any dependencies in the computation graph greater than length $T$.

As an example of BPTT, suppose we want to compute the gradient of an output neuron at timestep five with respect to an input at timestep zero, that is, $\frac{\partial \mathbf{x}_{\texttt{out}}[5]}{\partial \mathbf{x}_{\texttt{in}}[0]}$.  The forward pass (black arrows) and backward pass (red arrows) are shown in \fig{\ref{fig:recurrent_neural_nets:bptt1}}, where we have only drawn arrows for the subpart of the computation graph that is relevant to the calculation of this particular gradient.
\begin{figure}[h]
\centerline{
\begin{tikzpicture}%[>=spaced latex]
%
\def\Nnodes{6}
\def\Nnodesminusone{5}
\def\Nnodesminustwo{4}
\def\layerheight{1.0}
\def\neuronrad{0.2}
\def\neuronstep{1.1}
\foreach \x in {0,...,\Nnodes} {
    \draw (\neuronstep*\x-\neuronrad,0-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);
    \draw (\neuronstep*\x-\neuronrad,\layerheight-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);
    \draw (\neuronstep*\x-\neuronrad,\layerheight*2-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);
}
\foreach \x in {0,...,\Nnodesminustwo} {
    \draw [thick] [nn_edge] (\neuronstep*\x+\neuronrad,\layerheight) -- (\neuronstep*\x+\neuronstep-\neuronrad,\layerheight); \draw (\neuronstep*\x+\neuronrad+0.3,\layerheight+0.2) node {$\mathbf{W}$};
    \draw [thick,color=red] [nn_edge] (\neuronstep*\x+\neuronstep-\neuronrad,\layerheight-0.1) -- (\neuronstep*\x+\neuronrad,\layerheight-0.1);
}
\foreach \x in {0,...,\Nnodesminusone} {
    \draw [thick] [nn_edge] (\neuronstep*\x,\neuronrad) -- (\neuronstep*\x,\layerheight-\neuronrad); \draw (\neuronstep*\x+0.3,\neuronrad+0.3) node {$\mathbf{U}$};
}
\draw [thick] [nn_edge] (\neuronstep*5,\layerheight+\neuronrad) -- (\neuronstep*5,\layerheight*2-\neuronrad); \draw (\neuronstep*5+0.3,\layerheight+\neuronrad+0.3) node {$\mathbf{V}$};
%
\draw [thick] [nn_edge,color=red] (-0.1,\layerheight-\neuronrad) -- (-0.1,\neuronrad);
\draw [thick] [nn_edge,color=red] (\neuronstep*5-0.1,\layerheight*2-\neuronrad) -- (\neuronstep*5-0.1,\neuronrad+\layerheight);
%
\draw [] [times_arrow] (-\neuronrad,-0.4) -- (\neuronstep*\Nnodes+\neuronrad-0.7,-0.4);
\draw (\neuronstep*\Nnodes+\neuronrad-0.3,-0.4) node {\small{time}};
%
\draw (-0.9,0) node {$\xin$};
\draw (-0.9,\layerheight) node {$\mathbf{h}$};
\draw (-0.9,\layerheight*2) node {$\xout$};
\end{tikzpicture}
}
\caption{Backpropagation through time in an RNN.}
\label{fig:recurrent_neural_nets:bptt1}
\end{figure}

What about the gradient of the total cost with respect to the parameters, $\frac{\partial J}{\partial [\mathbf{W}, \mathbf{U}, \mathbf{V}]}$? Suppose the RNN is being applied to a video and predicts a class label for every frame of the video. Then $\xout$ is a prediction for each frame and a loss can be applied to each $\xoutt$. The total cost is simply the summation of the losses at each timestep:
\begin{align}
    \frac{\partial J}{\partial [\mathbf{W}, \mathbf{U}, \mathbf{V}]} = \sum_{t=0}^T \frac{\partial \mathcal{L}(\xoutt, \mathbf{y}_t)}{\partial [\mathbf{W}, \mathbf{U}, \mathbf{V}]}
\end{align}
The graph for backpropagation, shown in \fig{\ref{fig:recurrent_neural_networks:bptt_J}}, has a branching structure, which we saw how to deal with in \chap{\ref{chapter:backpropagation}}: gradients (red arrows) summate wherever the forward pass (black arrows) branches. 
\begin{figure}[h]
\centerline{
\begin{tikzpicture}%[>=spaced latex]
%
\def\Nnodes{6}
\def\Nnodesminusone{5}
\def\layerheight{1.0}
\def\neuronrad{0.2}
\def\neuronstep{1.1}
\foreach \x in {0,...,\Nnodes} {
    \draw (\neuronstep*\x-\neuronrad,0-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);
    \draw (\neuronstep*\x-\neuronrad,\layerheight-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);
    \draw (\neuronstep*\x-\neuronrad,\layerheight*2-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);
    \draw [thick] [nn_edge] (\neuronstep*\x,\neuronrad) -- (\neuronstep*\x,\layerheight-\neuronrad); \draw (\neuronstep*\x+0.3,\neuronrad+0.3) node {$\mathbf{U}$};
    \draw [thick] [nn_edge] (\neuronstep*\x,\layerheight+\neuronrad) -- (\neuronstep*\x,\layerheight*2-\neuronrad); \draw (\neuronstep*\x+0.3,\layerheight+\neuronrad+0.3) node {$\mathbf{V}$};
    \draw [thick] [nn_edge,color=red] (\neuronstep*\x-0.1,\layerheight-\neuronrad) -- (\neuronstep*\x-0.1,\neuronrad); 
    \draw [thick] [nn_edge,color=red] (\neuronstep*\x-0.1,\layerheight*2-\neuronrad) -- (\neuronstep*\x-0.1,\neuronrad+\layerheight);
    % 
    \draw [thick] [nn_edge] (\neuronstep*\x,\layerheight*2+\neuronrad) -- (\neuronstep*\x,\layerheight*3-\neuronrad); \draw (\neuronstep*\x,\layerheight*3) node {$\mathcal{L}$};
    \draw [thick] [nn_edge,color=red] (\neuronstep*\x-0.1,\layerheight*3-\neuronrad) -- (\neuronstep*\x-0.1,\layerheight*2+\neuronrad);
    %
    \draw [thick] [nn_edge] (\neuronstep*\x,\layerheight*3+\neuronrad) -- (\neuronstep*3+0.1*\neuronstep*\x-3*0.1*\neuronstep,\layerheight*4.5-\neuronrad);
    \draw [thick,color=red] [nn_edge] (\neuronstep*3+0.1*\neuronstep*\x-3*0.1*\neuronstep-0.1,\layerheight*4.5-\neuronrad) -- (\neuronstep*\x-0.1,\layerheight*3+\neuronrad);
}
\foreach \x in {0,...,\Nnodesminusone} {
    \draw [thick] [nn_edge] (\neuronstep*\x+\neuronrad,\layerheight) -- (\neuronstep*\x+\neuronstep-\neuronrad,\layerheight); \draw (\neuronstep*\x+\neuronrad+0.3,\layerheight+0.2) node {$\mathbf{W}$};
    \draw [thick,color=red] [nn_edge] (\neuronstep*\x+\neuronstep-\neuronrad,\layerheight-0.1) -- (\neuronstep*\x+\neuronrad,\layerheight-0.1);
}
%
\draw (\neuronstep*3,\layerheight*4.8-\neuronrad) node {$\sum$};
\draw [thick] [nn_edge] (\neuronstep*3,\layerheight*4.7+\neuronrad) -- (\neuronstep*3,\layerheight*5.6-\neuronrad);
\draw [thick] [nn_edge,color=red] (\neuronstep*3-0.1,\layerheight*5.6-\neuronrad) -- (\neuronstep*3-0.1,\layerheight*4.7+\neuronrad);
\draw (\neuronstep*3,\layerheight*5.8-\neuronrad) node {$J$};
%
\draw [] [times_arrow] (-\neuronrad,-0.4) -- (\neuronstep*\Nnodes+\neuronrad-0.7,-0.4);
\draw (\neuronstep*\Nnodes+\neuronrad-0.3,-0.4) node {\small{time}};
%
\draw (-0.9,0) node {$\xin$};
\draw (-0.9,\layerheight) node {$\mathbf{h}$};
\draw (-0.9,\layerheight*2) node {$\xout$};
\end{tikzpicture}
}
\caption{Backpropagation through time to minimize the sum of losses incurred at each timestep.}
\label{fig:recurrent_neural_networks:bptt_J}
\end{figure}

\subsection{The Problem of Exploding and Vanishing Gradients}
\label{sec:recurrent_neural_networks:bptt} 

Notice that the we can get very large computation graphs when we run an RNN for many timesteps. The gradient computation involves a series of matrix multiplies that is $\mathcal{O}(T)$ in length. For example, to compute our gradient $\frac{\partial \mathbf{x}_{\texttt{out}}[5]}{\partial \mathbf{x}_{\texttt{in}}[0]}$ in the example above involves the following product: 
\begin{align}
\frac{\partial \xoutt}{\partial \mathbf{x}_{\texttt{in}}[0]} = \frac{\partial \xoutt}{\partial \mathbf{h}_T} \frac{\partial \mathbf{h}_T}{\partial \mathbf{h}_{T-1}} \cdots \frac{\partial \mathbf{h}_1}{\partial \mathbf{h}_0} \frac{\partial \mathbf{h}_0}{\partial \mathbf{x}_{\texttt{in}}[0]}
\end{align}
Suppose we use an RNN with relu nonlinearities. Then each term $\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{t-1}}$ equals $\mathbf{R}\mathbf{W}$, where $\mathbf{R}$ is a diagonal matrix with zeros or ones on the $i$-th diagonal element depending on whether the $i$-th neuron is above or below zero at the current operating point. This gives a product $\mathbf{R}_T\mathbf{W}\cdots\mathbf{R}_1\mathbf{W}$. In a worst-case scenario (for numerics), suppose all the $\texttt{relu}$ are active; then we have a product of $T$ matrices $\mathbf{W}$, which means the gradient is $\mathbf{W}^T$ ($T$ is an exponent here, not a transpose). If values in $\mathbf{W}$ are large, the gradient will explode. If they are small, the gradient will vanishâ€”basically, this system is not numerically well-behaved. One solution to this problem is to use RNN units that have better numerics, an example of which we will give in \sect{\ref{sec:recurrent_neural_networks:LSTMS}} subsequently.

\section{Stacking Recurrent Layers}
A deep RNN can be constructed by stacking recurrent layers. Here is an example:
\begin{figure}[h]
\centerline{
\begin{tikzpicture}
%
\def\Nnodes{6}
\def\Nnodesminusone{5}
\def\layerheight{0.85}
\def\neuronrad{0.2}
\def\neuronstep{1.0}
\foreach \x in {0,...,\Nnodes} {
    \draw (\neuronstep*\x-\neuronrad,0-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);
    \draw (\neuronstep*\x-\neuronrad,\layerheight-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);
    \draw (\neuronstep*\x-\neuronrad,\layerheight*4-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);
    \draw (\neuronstep*\x-\neuronrad,\layerheight*5-\neuronrad) rectangle ++(\neuronrad*2,\neuronrad*2);
}
%
\draw [thick] [nn_edge] (\neuronstep,\layerheight*3+\neuronrad) -- (\neuronstep,\layerheight*4-\neuronrad); \draw (\neuronstep+0.4,\layerheight*3+\neuronrad+0.2) node {$\mathbf{U}_L$};
\draw [thick] [nn_edge] (\neuronstep,\neuronrad+\layerheight*4) -- (\neuronstep,\layerheight*5-\neuronrad); \draw (\neuronstep+0.3,\layerheight*4+\neuronrad+0.2) node {$\mathbf{V}$};
\draw [thick] [nn_edge] (\neuronrad,\layerheight*4) -- (\neuronstep-\neuronrad,\layerheight*4); \draw (\neuronrad+0.3,\layerheight*4+0.3) node {$\mathbf{W}_L$};
%
\draw [thick] [nn_edge] (\neuronstep,\neuronrad) -- (\neuronstep,\layerheight-\neuronrad); \draw (\neuronstep+0.4,\neuronrad+0.2) node {$\mathbf{U}_1$};
\draw [thick] [nn_edge] (\neuronstep,\neuronrad+\layerheight) -- (\neuronstep,\layerheight*2-\neuronrad); \draw (\neuronstep+0.4,\layerheight+\neuronrad+0.2) node {$\mathbf{U}_2$};
\draw [thick] [nn_edge] (\neuronrad,\layerheight) -- (\neuronstep-\neuronrad,\layerheight); \draw (\neuronrad+0.3,\layerheight+0.3) node {$\mathbf{W}_1$};
%
\draw [] [times_arrow] (-\neuronrad,-0.4) -- (\neuronstep*\Nnodes+\neuronrad-0.7,-0.4);
\draw (\neuronstep*\Nnodes+\neuronrad-0.3,-0.4) node {\small{time}};
%
\draw (-0.9,0) node {$\xin$};
\draw (-0.9,\layerheight) node {$\mathbf{h}_1$};
\draw (0,\layerheight*2.5) node {$\vdots$};
\draw (-0.9,\layerheight*4) node {$\mathbf{h}_L$};
\draw (-0.9,\layerheight*5) node {$\xout$};
\end{tikzpicture}
}
\caption{A deep RNN with multiple hidden layers.}
\end{figure}

To emphasize that the parameters are shared, we only show them once.

Here is code for a two-layer version of this RNN:
\begin{figure}[h]
\begin{minipage}{1.0\linewidth}
\begin{minted}[xleftmargin=0.085\linewidth,xrightmargin=0.085\linewidth,
fontsize=\fontsize{8.5}{9},
frame=single,
framesep=2.5pt,
baselinestretch=1.05,
]{python}
# d_in, d_hid, d_out: input, hidden, and output dimensionalities
# x : input data sequence
# h_init: initial conditions of hidden state
# T: number of timesteps to run for

# first define parameterized layers
U1 = nn.fc(dim_in=d_in, dim_out=d_hid, bias=False)
U2 = nn.fc(dim_in=d_hid, dim_out=d_hid, bias=False)
W1 = nn.fc(dim_in=d_hid, dim_out=d_hid)
W2 = nn.fc(dim_in=d_hid, dim_out=d_hid)
V = nn.fc(dim_in=d_hid, dim_out=d_out)

# then run data through network
h1, h2 = h_init
for t in range(T):
    h1[t] = nn.tanh(W1(h1[t-1])+U1(x[t]))
    h2[t] = nn.tanh(W2(h2[t-1])+U2(h1[t]))
    x_out[t] = V(h2[t])
\end{minted}
\end{minipage}
\end{figure}

We set the bias to be ``False'' for $\mathbf{U}_1$ and $\mathbf{U}_2$ since the recurrent layer already gets a bias term from $\mathbf{W}_1$ and $\mathbf{W}_2$.

%\section{Topologies for sequence prediction}


%\section{Efficiency trick: partial prediction}


\section{Long Short-Term Memory}\label{sec:recurrent_neural_networks:LSTMS}
\index{Long Short-Term Memory}{\bf Long Short-Term Memory layers} ({\bf LSTMs}) are a special kind of recurrent module, that is designed to avoid the vanishing and exploding gradients problem described in \sect{\ref{sec:recurrent_neural_networks:bptt}} ~\cite{hochreiter1997long}.\marginnote{The presentation of LSTMs in this section draws inspiration from a great blog post by Chris Olah \cite{olah_lstms}.}

An LSTM is like a little memory controller. It has a \textbf{cell state}, $\mathbf{c}$, which is a vector it can read from and write to. The cell state can record memories and retrieve them as needed. The cell state plays a similar role as the hidden state, $\mathbf{h}$, from regular RNNsâ€”both record memoriesâ€”but the cell state is built to store \textit{persistent} memories that can last a long time (hence the name \textit{long} short-term memory). We will see how next.

Like a normal recurrent layer, an LSTM takes as input a hidden state $\mathbf{h}_{t-1}$ and an observation $\xin[t]$ and produces as output an updated hidden state $\mathbf{h}_t$. Internally, the LSTM uses its cell state to decide how to update $\mathbf{h}_{t-1}$.

First, the cell state is updated based on the incoming signals, $\mathbf{h}_{t-1}$ and $\xin[t]$. Then the cell state is used to determine the hidden state $\mathbf{h}_{t}$ to output. Different subcomponents of the LSTM layer decide what to \textit{delete} from the cell state, what to \textit{write} to the cell state, and what to \textit{read} off the cell state. 

One component picks the indices of the cell state to \textit{delete}:
\begin{align}
    \mathbf{f}_t &= \sigma(\mathbf{W}_f\mathbf{h}_{t-1} + \mathbf{U}_f\xin[t] + \mathbf{b}_f) &\triangleleft \quad\quad\text{decide which indices to forget}
\end{align}
where $\mathbf{f}_t$ is a vector of length equal to the length of the cell state $\mathbf{c}_t$, and $\sigma$ is the sigmoid function that outputs values in the range $[0,1]$. The idea is that most values in $\mathbf{f}_t$ will be near either $1$ (remember) or 0 (forget). Later $\mathbf{f}_t$ will be pointwise multiplied by the cell state to forget the values where $\mathbf{f}_t$ is zero.

Another component chooses what values to \textit{write} to the cell state, and where to write them:
\begin{align}
    \mathbf{i}_t &= \sigma(\mathbf{W}_i\mathbf{h}_{t-1} + \mathbf{U}_i\xin[t] + \mathbf{b}_i) &\triangleleft \quad\quad\text{which indices to write to}\\
    \tilde{\mathbf{c}}_t &= \texttt{tanh}(\mathbf{W}_C\mathbf{h}_{t-1} + \mathbf{U}_c\xin[t] + \mathbf{b}_c) &\triangleleft \quad\quad\text{what to write to those indices}
\end{align}
where $\mathbf{i}_t$ is similar to $\mathbf{f}_t$ (it's a vector whose values that are nearly 0 or 1 of length equal to the length of the cell state) and it determines which indices of $\tilde{\mathbf{c}}_t$ will be written to the cell state. Next we use these vectors to actually update the cell state:
\begin{align}
    \mathbf{c}_t &= \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_{t} &\triangleleft \quad\quad\text{update the cell state}
\end{align}

Finally, given the updated cell state, we \textit{read} from it and use the values to determine the hidden state to output:
\begin{align}
    \mathbf{o}_t &= \sigma(\mathbf{W}_o\mathbf{h}_{t-1} + \mathbf{U}_o\xin[t] + \mathbf{b}_o) &\triangleleft \quad\quad\text{which indices of cell state to use}\\
    \mathbf{h}_t &= \mathbf{o}_t \odot \texttt{tanh}(\mathbf{c}_t) &\triangleleft \quad\quad\text{use these to determine next hidden state}
\end{align}
The basic idea is it should be easy, by default, not to forget. The cell state controls what is remembered. It gets modulated, but $\mathbf{f}_t$ can learn to be all ones so that information propagates forward through an identity connection from the previous cell state to the subsequent cell state. This idea is similar to the idea of skip connections and residual connections in U-Nets and ResNets (\chap{\ref{chapter:convolutional_neural_nets}}).


\section{Concluding Remarks}
Recurrent connections give neural nets the ability to maintain a persistent state representation, which can be propagated forward in time or can be updated in the face of incoming experience. The same idea has been proposed in multiple fields: in control theory it is the basic distinction between open-loop and closed-loop control. %, and this is the same as the distinction between feedforward and recurrent nets. 
Closed-loop control allows the controller to change its behavior based on \textit{feedback} from the consequences of its previous decisions; similarly, recurrent connections allow a neural net to change its behavior based on feedback, but the feedback is from downstream processing all within the same neural net. Feedback is a fundamental mechanism for making systems that are adaptive. These systems can become more competent the longer you run them.
