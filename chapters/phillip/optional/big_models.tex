\chapter{Big models}

CLIP
StyleGAN
DALL-E
BigGAN
...

Like any field, computer vision goes through different fashions, and the fashions repeat. One of the periodic cycles is about how much we make use of pre-existing systems when trying to solve a new problem.

In the early days, before the 1980s or so, there were no pre-existing systems, so new systems were built from scratch for each proposed problem: we got basic edge detectors, corner detectors, etc and they were all separate. Then these tools became standard modules, and in the 1980s through 2000s, you would be considered quite eccentric indeed if you did not start with them. Computer vision systems of that era would call Canny when they needed edges, or SIFT when they wanted feature descriptors. These modules were used off the shelf without tuning. They were prior knowledge that could be drawn upon to solve new problems.

Starting in the 2000s and then especially in the 2010s, the pendulum swung in the other direction and researchers and practitioners embraced ``end-to-end" learning. Rather than using SIFT for your problem, it was better to train a new feature extractor jointly with the rest of your system. The hybrid idea of transfer learning also became popular in this era but it was a kind of transfer where you essentially use an old system as the init for your new model, and then extensively train, end-to-end, beyond that point.

Now we seem to be entering another transition back to reliance on big pre-trained systems. We still do transfer learning in this era but the finetuning only minimally modifies the pre-existing system and often it is not end-to-end. The main reason for this is that the pre-existing systems are very big right now, and few people have the resources to signficantly change them.

This is like in humans how our learning during adulthood does not significantly change our basic sensory and motor systems, nor our huge base of knowledge acquired over our lifetimes. Instead the tuning is just a little bit on the top each day, and not necessarily end-to-end. That's because it took about 18 years to reach adulthood and one couldn't afford to repeat that process many times.

These cycles are perhaps driven by a kind of economics. When the relative cost of building functionality is low compared to the functional capabilities of pre-existing systems, then we build anew each time, tailored to each problem. When the cost is high, we rely more on prior systems. Human-like AI will likely have high cost of retraining so we might expect that the current era is here to stay: from now on we will always be minimally tuning big pre-trained models. But one could also imagine that compute costs become so low that it actually is not very costly to recreate a human-level AI from scratch, and then the cycle could repeat, until the next stage of superintelligent and even bigger AI systems reverses the cycle again.

Anyway, an important paradigm right now is making use of huge pretrained systems, with minimal modification, and we will devote this chapter to studying how to work within that setting.

