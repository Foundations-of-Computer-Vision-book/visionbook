# Temporal Filters {#chapter-temporal_filters}

## Introduction

Although adding time might seem like a trivial extension from
two-dimensional (2D) signals to three-dimensional (3D) signals, and in
many aspects it is, there are some properties of how the world behaves
that make sequences seem different from arbitrary 3D signals. In 2D
images most objects are bounded, occupying compact and well defined
image regions. However, in sequences, objects do not appear and
disappear instantaneously unless they get occluded behind other objects
or enter or exit the scene through doors or the image boundaries. So,
the behavior of an object across time $t$ is very different than its
behavior across space $n,m$. In time, objects move and deform defining
continuous trajectories that have no beginning and never end.

## Modeling Sequences {#sect:modelingSequences}

Sequences will be represented as functions $\ell(x,y,t)$, where $x,y$
are the spatial coordinates and $t$ is time. As before, when processing
sequences we will work with the discretized version that we will
represent as $\ell\left[n,m,t \right]$, where $n,m$ are the pixel
indices and $t$ is the frame number. Discrete sequences will be bounded
in space and time, and can be stored as arrays of size
$N \times M \times P$.

@fig-motion illustrates this with one sequence, as shown in
@fig-motiona. This sequence has 90 frames and shows people on the
street walking parallel to the camera plane and at different distances
from the camera. @fig-motionb shows the space-time array
$\ell\left[n,m,t \right]$. When we look at a picture we are a looking at
a 2D section, $t=constant$, of this cube. But it is interesting to look
at sections along other orientations. Figures
[\[fig:motion\]](#fig-motion){reference-type="ref"
reference="fig:motion"}(c-d) show sections for $m=constant$ and
$n=constant$, respectively. Although they are also 2D images, their
structure looks very different from the images we are used to seeing.
@fig-motionc shows a horizontal section that is parallel to the
direction of motion of the people walking. Here we see straight bands
with different orientations. The bands appear to occlude each other.
Each band corresponds to one person and its orientation is given by the
speed and the direction of motion. @fig-motiond looks like a
photo-finish picture like the ones used in sporting races. In both
figures [\[fig:motion\]](#fig-motion){reference-type="ref"
reference="fig:motion"}(c and d), static objects appear as vertical
stripes in @fig-motionb, and horizontal stripes in @fig-motiond.

![](figures/temporal_filters/motion_illustration.png){width="100%"}

One special sequence is when the image has a global motion with constant
velocity $(v_x,v_y)$. In such a case we can write:
$$\ell(x,y,t) = \ell_0 (x-v_x t,y-v_y t)$${#eq-globaly_moving_image} where $\ell_0(x,y)= \ell(x,y,0)$ is the image
being translated, and $v_x$ and $v_y$ are constants. At time $t$ the
image $\ell_0(x,y)$ is translated by the vector $(v_x t,v_y t)$ as
described by equation (@eq-globaly_moving_image). The pixel value at
location $(x=0, y=0)$ at time $t=0$ will appear at time $T$ in location
$(x=v_x T, y=v_y T)$. This is what we see in @fig-motionc where the
bands are created by moving pixels.

We use continuous values for $x$, $y$, and $t$, and continuous images,
$\ell_0(x,y)$, because it allows us to deal with any velocity values.
This function also assumes that the brightness of the pixels does not
change while the scene is moving (**constant brightness assumption**).

In general, sequences will be more complex, but the properties of a
globally moving image are helpful to understand local properties in
sequences. We can also write models for more complex sequences. For
instance, a sequence containing a moving object over a static background
can be written as:
$$\ell(x,y,t) = b(x,y) (1-m(x-v_xt,y-v_yt)) + o(x-v_xt,y-v_yt) m(x-v_x t,y-v_y t)$$where $b(x,y)$ is the static background image, $o(x,y)$ is the object
image moving with speed $(v_x,v_y)$, and $m(x,y)$ is a binary mask that
moves with the object and that models the fact that the object occludes
the background.

## Modeling Sequences in the Fourier Domain {#section:Modeling_sequences_in_the_Fourier_domain}

The Fourier transform (FT) of a globally moving image, equation
(@eq-globaly_moving_image), is (using the shift property):
$$\mathscr{L}(w_x,w_y,w_t) = \mathscr{L}_0 (w_x, w_y) \delta (w_t + v_x w_x + v_y w_y)$$
The continuous FT of the sequence is equal to the product of the 2D FT
of the static image $\ell_0(x,y)$ and a delta wall. To better understand
this function let's look at a simple example on only one spatial
dimension, as shown in @fig-mov_pulse_012.

![](figures/temporal_filters/mov_pulse_012.png){width="100%"}

@fig-mov_pulse_012 shows the FT for a sequence with one spatial
dimension, $\ell(x,t)$, that contains a blurry rectangular pulse moving
at three different speeds toward the left. @fig-mov_pulse_012a shows a
sequence when the pulse is static and @fig-mov_pulse_012d shows its FT.
Across the spatial frequency $w_x$, the FT is approximately a sinc
function. Across the temporal frequency $w_t$, as the signal is
constant, its FT is a delta function. Therefore the FT is a sinc
function contained inside a delta wall in the line $w_t=0$.
@fig-mov_pulse_012b shows the same rectangular pulse moving towards the
left, $v_x=0.5$. @fig-mov_pulse_012e shows the sinc function but skewed
a long the frequency line $w_t+0.5w_x=0$. Note that this is not a
rotation of the sinc function from @fig-mov_pulse_012d as the locations
of the zeros lie at the same $w_x$ locations. @fig-mov_pulse_012c shows
the pulse moving at a faster speed resulting in a larger skewing of its
FT, @fig-mov_pulse_012f.

## Temporal Filters {#temporal-filters}

Linear spatiotemporal filters can be written as spatiotemporal
convolutions between the input sequence and a convolutional kernel
(impulse response). Discrete spatiotemporal filters have an impulse
response $h \left[n,m,t \right]$, where the independent variables $n$,
$m$, and $t$ are discrete and only take on integer values. The extension
from 2D filter to spatiotemporal filters does not has any additional
complications. We can also classify filters as low-pass, high-pass, and
so on. But in the case of time, there is another attribute used to
characterize filters, namely **causality**:

-   Causal filters. These are filters with output variations that only
    depend of the past values of the input. This puts the following
    constraint: $h \left[n,m,t \right]=0$ for all $t<0$. This means that
    if the input is an impulse at $t=0$, the output will only have
    non-zero values for $t>0$. If this condition is satisfied, then the
    filter output will only depend on the input's past for all possible
    inputs.

-   Noncausal filters. When the output has dependency on future inputs.

-   Anticausal filters. This is the opposite, when the output only
    depends on the future: $h \left[n,m,t \right]=0$ for all $t>0$.

Many filters are noncausal and have both causal and anticausal
components (e.g., a Gaussian filter). Note that noncausal filters can
not be implemented in practice and, therefore, any filter with an
anticausal component will have to be approximated by a purely causal
filter by bounding the temporal support and shifting in time the impulse
response.

In this chapter, we have written all the filters as convolutions.
However, some filters are better described as difference equations (this
is specially important in time). An example of a difference equation is:
$$\ell_{\texttt{out}}\left[n,m,t \right] = \ell_{\texttt{in}}\left[n,m,t \right] + \alpha \, \ell_{\texttt{out}}\left[n,m,t-1 \right]$$
where the output $\ell_{\texttt{out}}$ at time $t$ depends on the input
at time $t$ and the output at the previous time instant $t-1$ multiplied
by a constant $\alpha$. We can easily evaluate the impulse response,
$h \left[n,m,t \right]$, of such a filter by replacing
$\ell_{\texttt{in}}\left[n,m,t \right]$ with an impulse,
$\delta \left[n,m,t \right]$. 

:::{.column-margin}
Remember that
$\delta \left[n,m,t \right]$ is 1 for $n=0,m=0,t=0$, and 0 everywhere
else.
:::

 The impulse response is:
$$h \left[n,m,t \right] = \alpha^t  \delta \left[n,m \right] u \left[t \right]$$
where $u\left[t \right]$, called the **Heaviside step** function, is:
$$u \left[t \right] = \begin{cases}
    0     & \quad \text{if }  t <0 \\
    1     & \quad \text{otherwise }\\
\end{cases}$$ 

Most filters described by differences equations have an impulse response
with infinite support. They are called Infinite Impulse Response (IIR)
filters. IIR filters can be further classified as stable and unstable.
**Stable** are the ones that given a bounded input,
$| \ell_{\texttt{in}}\left[n,m,t \right] |<A$, produce a bounded output,
$| \ell_{\texttt{out}}\left[n,m,t \right] | <B$. For this to happen, the
impulse response has to be bounded. In **unstable** filters, the
amplitude of the impulse response diverges to infinity. In the previous
example, the filter is stable if and only if $| \alpha | < 1$.

Let's now describe some spatiotemporal filters.

### Spatiotemporal Gaussian

As with the spatial case, we can define the same low-pass filters in the
spatiotemporal domain: the box filter, triangular filters, and so on. As
an example, let's focus on the Gaussian filter. The spatiotemporal
Gaussian is a simple extension of the spatial Gaussian filter in
sectionÂ [\[sec-spt_gaussian\]](#sec:spt_gaussian){reference-type="ref"
reference="sec-spt_gaussian"}:
$$g(x,y,t; \sigma_x,\sigma_t) = \frac{1}{(2 \pi)^{3/2} \sigma_x^2\sigma_t} \exp{-\frac{x^2 +
   y^2}{2 \sigma_x^2}} \exp{-\frac{t^2}{2 \sigma_t^2}}
$${#eq-gauss3dcont} where $\sigma_x$ is the width of the Gaussian along
the two spatial dimensions, and $\sigma_t$ is the width on the temporal
domain. As the units for $t$ and $x,y$ are unrelated, it does not make
sense to set all the $\sigma$s to have the same value.

We can discretize the continuous Gaussian by taking samples and building
a 3D convolutional kernel. We can also use the binomial approximation.
The 3D Gaussian is separable so it can be implemented efficiently as a
convolutional cascade of three one-dimensional (1D) kernels.
@fig-seq_filtered_kernela shows a spatiotemporal Gaussian. The temporal
Gaussian is a noncausal filter, therefore it is not physically
realizable. This is not a problem when processing a video stored in
memory. However, if we are processing a streamed video, we will have to
bound and shift the filter to make it causal, which will result in a
delay in the output.

![](figures/temporal_filters/seq_filtered_kernel.png){width="100%"}

@fig-sec_filtered_blura shows one sequence and @fig-sec_filtered_blurb
shows the sequence filtered with the Gaussian from
@fig-seq_filtered_kernela. This Gaussian has a small spatial width,
$\sigma=1$, and a large temporal width, $\sigma_t=4$ so the sequence is
strongly blurred across time. The moving objects show motion blur and
are strongly affected by the temporal blur, while the static background
is only affected by the spatial width of the Gaussian.

![](figures/temporal_filters/seq_filtered_blur.png){width="100%"}

How could we create a filter that keeps sharp objects that move at some
velocity $(v_x,v_y)$ while blurring the rest? @fig-sec_filtered_blurc
shows the desired output of such a filter. The bottom image shows one
frame for a sequence filtered with a kernel that keeps sharp objects
moving left at 1 pixel/frame while blurring the rest. This filter can be
obtained by skewing the Gaussian:
$$g_{v_x,v_y}(x,y,t) = g(x - v_xt,y - v_yt, t)$$ This directional bluris not a rotation of the original Gaussian as the change of variables is
not unitary, but the same effect could be obtained with a rotation.
@fig-sec_filtered_blurc shows the effect when $v_x=-1, v_y=0$. The
Gaussian is shown in @fig-seq_filtered_kernelb. The space-time section
shows how the sequence is blurred everywhere except one oriented bad
corresponding to the person walking left. @fig-sec_filtered_blurd shows
the effect when $v_x=1, v_y=0$. The output of this filter looks as if
the camera was tracking one of the objects while the shutter was open,
producing a blurry image of all the other objects.

### Temporal Derivatives

Spatial derivatives are useful to find regions of image variation such
as object boundaries. Temporal derivatives can be used to locate moving
objects. We can approximate a temporal derivative for discrete signals
as the difference: $$\ell\left[m,n,t\right] - \ell\left[m,n,t-1\right]$$:::{.column-margin}
When implementing temporal filters it is important to
use causal filters. A causal filter depends only on the present and past
input samples.
:::



As in the spatial case, it is useful to compute temporal derivatives of
spatiotemporal Gaussians:
$$\frac{\partial g}{\partial t} = \frac{-t}{\sigma_t^2} g(x,y,t)$$ where
$g(x,y,t)$ is the Gaussian as written in equation (@eq-gauss3dcont). We
can compute the spatiotemporal gradient of a Gaussian: $$\begin{aligned}
\nabla  g &=& \left( g_x(x,y,t), \, g_y(x,y,t), \, g_t(x,y,t) \right) \nonumber \\
 &=&  \left(-x/\sigma^2, -y/\sigma^2, -t/\sigma_t^2 \right) g(x,y,t)
 
\end{aligned}$${#eq-spt_gradient_gaussian} We can use the analytic form of the spatiotemporal
Gaussian derivatives from equation (@eq-spt_gradient_gaussian) to
discretize the filter, by taking samples at discrete locations in space
and time, and use the resulting discrete spatiotemporal kernel to filter
an input discrete sequence. These filters can be used for many
applications. Let's look at one practical example: What should we do if
we want to remove from a sequence the objects moving at a particular
velocity?

To answer that question, we will first assume the sequence contains a
single object moving at a constant velocity. We will then compute the
derivatives along $x$, $y$, and $t$ of the sequence and we will find out
what particular linear combination of those derivatives makes the output
go to zero only when the input sequence moves at a target velocity.

In the case of an image, $\ell_0 (x,y)$, that moves with velocity
$(v_x, v_y)$, the sequence is $$\ell(x,y,t) = \ell_0 (x-v_xt,y-v_yt)$$We can compute the temporal derivative of $\ell(x,y,t)$ as:
$$\frac{\partial \ell}{\partial t} = \frac{\partial \ell_0}{\partial t} = -v_x \frac{\partial \ell_0}{\partial x} - v_y \frac{\partial \ell_0}{\partial y}
$${#eq-brightnessconstancy} This result is interesting because it shows
that for a moving image there is a relationship between the temporal
derivative of the sequence and the spatial derivatives a long the
direction of motion. We can use this relationship to find a linear
combination of the temporal and spatial derivatives so that the output
is zero.

In fact, if we compute the gradient of the Gaussian along the vector
$\left[1,v_x,v_y\right]$:
$$h(x,y,t;v_x,v_y) = g_t+v_xg_x+v_yg_y = \nabla  g \left( 1,v_x,v_y \right)^\mathsf{T}$${#eq-spt_nulling_filter} we get a kernel $h(x,y,t;v_x,v_y)$ that we can
use as a spatiotemporal filter that will do exactly what we were looking
for. Indeed, if we convolve it with the input sequence
$\ell_0 (x-v_xt,y-v_yt)$ we get a zero output (using equation
\[@eq-brightnessconstancy\]): $$\begin{aligned}
\ell_0 (x-v_xt,y-v_yt) \circ h &=& \ell_0 (x-v_xt,y-v_yt) \circ \left(  g_t+v_xg_x+v_yg_y \right) \nonumber \\
&=& \left( \frac{\partial \ell_0}{\partial t} + v_x \frac{\partial \ell_0}{\partial x} + v_y \frac{\partial \ell_0}{\partial y} \right) \circ g \nonumber \\
&=& 0 \nonumber
\end{aligned}$$

The filter $h$ from equation (@eq-spt_nulling_filter) is shown in
@fig-gaussian_seq. As this filter is 3D we show as a sequence for
different velocities. Each row in the figure corresponds to one
particular velocity $v_x,v_y$. In the example shown in
@fig-gaussian_seqa the Gaussian has a width of
$\sigma^2=\sigma_t^2=1.5$, and has been discretized as a 3D array of
size $7 \times 7 \times 7$. Figures
[\[fig:gaussian_seq\]](#fig-gaussian_seq){reference-type="ref"
reference="fig:gaussian_seq"}(b,c and d) show the filter $h$ for
different velocities: $(v_x,v_y) =$ $(0,0)$, $(1,0)$ and $(-1,0)$.

![](figures/temporal_filters/gaussians_xyt_seq.png){width="100%"}

@fig-gaussian_xyt_section shows a different visualization of the same
filter $h$ from equation (@eq-spt_nulling_filter). Each image in
@fig-gaussian_xyt_section shows a space-time section of the same
spatiotemporal Gaussian derivatives as the ones shown in
@fig-gaussian_seq. Both visualizations are equivalent and help to
understand how the filter works.

![](figures/temporal_filters/gaussians_xyt_section.png){width="100%"}

Such a filter $h$ will cancel any objects moving at the velocity
$(v_x,v_y)$. By using different filters, each one computing derivatives
a long different space-time orientations, we can create output sequences
where specific objects disappear, as shown in @fig-tunedfilter. This
filter is called a **nulling filter** @Darrell93.

![](figures/temporal_filters/seq_filtered_der.png){width="100%"}

Note that despite that we assumed that the sequence contained a single
moving object when deriving the formulation for the nulling filter, the
filter also works in sequences with multiple objects moving a different
speeds as shown in @fig-tunedfilter. This is because the operations are
local (the kernel $h$ has a small size) and the behavior will be correct
if in a local image patch there is only one velocity present. In fact,
it is easy to show that the method will also work if the sequence is a
sum of several moving transparent layers.

## Concluding Remarks

In this chapter we have discussed different types of spatiotemporal
filters used to analyze video. However, we have not presented how these
filters can be used to estimate useful quantities such as velocity. We
will devote several chapters in part
[\[part:understanding_motion\]](#part:understanding_motion){reference-type="ref"
reference="part:understanding_motion"} to motion estimation.
