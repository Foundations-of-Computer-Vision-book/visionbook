# Temporal filters {#chapter-temporal_filters}

Although adding time might seem like a trivial extension from 2D signals to 3D signals, and in many aspects it is, there are some properties of how the world behaves that make sequences different from arbitrary 3D signals. In 2D images, most objects are bounded, occupying compact and well-defined image regions. However, in sequences, objects do not appear and disappear instantaneously unless they get occluded behind other objects or enter or exit the scene through doors or the image boundaries. So, the behavior of objects across time $t$ is very different than their behavior across space $n,m$. In time, objects move and deform, defining continuous trajectories that have no beginning and never end.

## Modeling sequences {#sect-modelingSequences}

Sequences will be represented as functions $f (x,y,t)$, where $x,y$ are the spatial coordinates and $t$ is time. As before, when processing sequences, we will work with the discretized version that we will represent as $f \left[n,m,t \right]$, where $n,m$ are the pixel indices and $t$ is the frame number. Discrete sequences will be bounded in space and time, and can be stored as arrays of size $N \times M \times P$.

![a) 8 Frames from a sequence with people walking. The frames are shown at regular time intervals. The full sequence had 90 frames (corresponding to 3 seconds of video). b) Space-time array, $f \left[n,m,t \right]$ of size $128 \times 128 \times 90$. c) Section for $m=50$, d) Section for $n=75$. Static objects appear as straight lines.](figures/temporal_filters/motion_illustration.png){#fig-motion}

@fig-motion illustrates this with one sequence shown in @fig-motion(a). This sequence has 90 frames and shows people on the street walking parallel to the camera plane and at different distances from the camera. @fig-motion(b) shows the space-time array $f \left[n,m,t \right]$. When we look at a picture, we are looking at a 2D section, $t=constant$, of this cube. But it is interesting to look at sections along other orientations.  @fig-motion(c) and @fig-motion(d) show sections for $m=constant$ and $n=constant$, respectively. Although they are also 2D images, their structure looks very different from the images we are used to seeing. @fig-motion(c) shows a horizontal section that is parallel to the direction of motion of the people walking. Here we see straight bands with different orientations. These bands appear to occlude each other. Each band corresponds to one person, and its orientation is given by the speed of walk and the direction of motion. @fig-motion(d) looks like a photo-finish photograph, similar to those used in sporting races. In both images (c) and (d), static objects appear as vertical stripes in (b) and horizontal stripes in (d).

One special sequence is when the image has a global motion with constant velocity $(v_x,v_y)$. In such a case, we can write:
$$
f (x,y,t) = f_0 (x-v_xt,y-v_yt)
$$
where $f_0(x,y)= f (x,y,0)$ is the image being translated, and $v_x$ and $v_y$ are constants. We use continuous functions because it allows us to deal with any velocity values. This function assumes also that the brightness of the pixels does not change while the scene is moving (**constant brightness assumption**).

In general, sequences will be more complex, but the properties of a globally moving image are helpful to understand local properties in sequences. We can also write models for more complex sequences. For instance, a sequence containing a moving object over a static background can be written as:
$$
f (x,y,t) = b(x,y) (1-m(x-v_xt,y-v_yt)) + o(x-v_xt,y-v_yt) m(x-v_x t,y-v_y t)
$$
where $b(x,y)$ is the static background image, $o(x,y)$ is the object image moving with speed $(v_x,v_y)$, and $m(x,y)$ is a binary mask that moves with the object and that models the fact that the object occludes the background.

## Modeling sequences in the Fourier domain

The FT of a globally moving image is (using the shift property):
$$
F (w_x,w_y,w_t) = F_0 (w_x, w_y) \delta (w_t + v_x w_x + v_y w_y)
$$
The continuous FT of the sequence is equal to the product of the 2D FT of the static image $f_0(x,y)$ and a delta wall. To better understand this function, let's look at a simple example in only one spatial dimension, as shown in @fig-mov_pulse_012.

![a) A sequence with one spatial dimension showing a static rectangular pulse. b) The rectangular pulse moves to the left at a speed $v=-0.5$ and c) moving towards the left, $v=-1$. As we work with discretized signals, speed units are in pixels per frame.](figures/temporal_filters/mov_pulse_012.png){#fig-mov_pulse_012}

@fig-mov_pulse_012 shows the FT for a sequence with one spatial dimension, $f(x,t)$, that contains a blurry rectangular pulse moving at three different speeds towards the left. @fig-mov_pulse_012(a) shows a sequence when the pulse is static, and @fig-mov_pulse_012(d) shows its FT. Across the spatial frequency $w_x$, the FT is approximately a sinc function. Across the temporal frequency $w_t$, as the signal is constant, its FT is a delta function. Therefore, the FT is a sinc function contained inside a delta wall in the line $w_t=0$. @fig-mov_pulse_012(b) shows the same rectangular pulse moving towards the left, $v_x=0.5$. @fig-mov_pulse_012(e) shows the sinc function but skewed along the frequency line $w_t+0.5w_x=0$. Note that this is not a rotation of the sinc function from @fig-mov_pulse_012(d) as the locations of the zeros lie at the same $w_x$ locations. @fig-mov_pulse_012(c) shows the pulse moving at a faster speed resulting in a larger skewing of its FT, @fig-mov_pulse_012(f).

We leave to the reader the work of computing its FT and visualizing the effect of the mask in the Fourier domain.

## Temporal filters

Linear spatio-temporal filters can be written as spatio-temporal convolutions between the input sequence and a convolutional kernel (impulse response). Discrete spatiotemporal filters have an impulse response $h \left[n,m,t \right]$. The extension from 2D filters to spatio-temporal filters does not have any additional complications. We can also classify filters as low-pass, high-pass, etc. But in the case of time, there is another attribute used to characterize filters: causality.

- **Causal filters**: These are filters with output variations that only depend on the past values of the input. This puts the following constraint: $h \left[n,m,t \right]=0$ for all $t<0$. This means that if the input is an impulse at $t=0$, the output will only have non-zero values for $t>0$. If this condition is satisfied, then the filter output will only depend on the input's past for all possible inputs.
- **Non-causal filters**: When the output has a dependency on future inputs.
- **Anti-causal filters**: This is the opposite, when the output only depends on the future: $h \left[n,m,t \right]=0$ for all $t>0$.

Many filters are non-causal and have both causal and anti-causal components (e.g., a Gaussian filter). Note that non-causal filters cannot be implemented in practice and, therefore, any filter with an anti-causal component will have to be approximated by a purely causal filter by bounding the temporal support and shifting in time the impulse response.

In this chapter, we have written all the filters as convolutions. However, some filters are better described as difference equations (this is especially important in time). An example of a difference equation is:
$$
g \left[n,m,t \right] = f \left[n,m,t \right] + \alpha \, h \left[n,m,t-1 \right]
$$
where the output $g$ at time $t$ depends on the input at time $t$ and the output at the previous time instant $t-1$ multiplied by a constant $\alpha$. We can easily evaluate the impulse response, $h \left[n,m,t \right]$, of such a filter by replacing $f\left[n,m,t \right]$ with an impulse, $\delta \left[n,m,t \right]$. The impulse response is:
$$
h \left[n,m,t \right] = \alpha^t  \delta \left[n,m \right] u \left[t \right]
$$
where $u\left[t \right]$, called the Heaviside step function, is:
$$
u \left[t \right] = \begin{cases}
    0     & \quad \text{if }  t <0 \\
    1     & \quad \text{otherwise }\\
\end{cases}
$$
Most filters described by difference equations have an impulse response with infinite support. They are called IIR (Infinite Impulse Response) filters. IIR filters can be further classified as stable and unstable. Stable filters are the ones that given a bounded input, $| f \left[n,m,t \right] |<A$, produce a bounded output, $| g \left[n,m,t \right] | <B$. For this to happen, the impulse response has to be bounded. In unstable filters, the amplitude of the impulse response diverges to infinity. In the previous example, the filter is stable if and only if $| \alpha | < 1$.

Let's now describe some spatio-temporal filters.

### Temporal Gaussian

As with the spatial case, we can define the same low-pass filters: the box filter, triangular filters, etc.  As an example, let's focus on the Gaussian filter. The spatio-temporal Gaussian is a trivial extension of the spatial Gaussian filter we have seen in \@sec-spt_gaussian:
$$
g(x,y,t; \sigma_x,\sigma_t) = \frac{1}{(2 \pi)^{3/2} \sigma_x^2\sigma_t} \exp{-\frac{x^2 + y^2}{2 \sigma_x^2}} \exp{-\frac{t^2}{2 \sigma_t^2}}
\label{eq-gauss3dcont}
$$
Where $\sigma_x$ is the width of the Gaussian along the two spatial dimensions, and $\sigma_t$ is the width in the temporal domain. As the units for $t$ and $x,y$ are unrelated, it does not make sense to set all the $\sigma$s to have the same value.

We can discretize the continuous Gaussian by taking samples and building a 3D convolutional kernel. We can also use the binomial approximation. The 3D Gaussian is separable, so it can be implemented efficiently as a convolutional cascade of 3 one-dimensional kernels. @fig-seq_filtered_kernel(a) shows a spatio-temporal Gaussian. The temporal Gaussian is a non-causal filter; therefore, it is not physically realizable. This is not a problem when processing a video stored in memory. However, if we are processing a streamed video, we will have to bound and shift the filter to make it causal, which will result in a delay in the output.

@fig-sec_filtered_blur(a) shows one sequence, and @fig-sec_filtered_blur(b) shows the sequence filtered with the Gaussian from @fig-seq_filtered_kernel(a). This Gaussian has a small spatial width, $\sigma=1$, and a large temporal width, $\sigma_t=4$, so the sequence is strongly blurred across time. The moving objects show motion blur and are strongly affected by the temporal blur, while the static background is only affected by the spatial width of the Gaussian.

![a) Spatio-temporal Gaussian with $\sigma=1$ and $\sigma_t=4$. b) Same Gaussian parameters but skewed by the velocity vector $v_x=-1, v_y=0$ pixels/frame, c) and $v_x=1, v_y=0$ pixel/frame.](figures/temporal_filters/seq_filtered_kernel.png){#fig-seq_filtered_kernel}

How could we create a filter that keeps sharp objects that move at some velocity $(v_x,v_y)$ while blurring the rest? @fig-sec_filtered_blur(c) shows the desired output of such a filter. The bottom image shows one frame for a sequence filtered with a kernel that keeps sharp objects moving left at 1 pixel/frame while blurring the rest. This filter can be obtained by skewing the Gaussian:
$$
g_{v_x,v_y}(x,y,t) = g(x - v_xt,y - v_yt, t)
$$
This directional blur is not a rotation of the original Gaussian as the change of variables is not unitary, but the same effect could be obtained with a rotation. @fig-sec_filtered_blur(c) shows the effect when $v_x=-1, v_y=0$. The Gaussian is shown in @fig-seq_filtered_kernel(b). The space-time section shows how the sequence is blurred everywhere except one oriented band corresponding to the person walking left. @fig-sec_filtered_blur(d) shows the effect when $v_x=1, v_y=0$. The output of this filter looks as if the camera was tracking one of the objects while the shutter was open, producing a blurry image of all the other objects.

![a) One frame from the input sequence and the space-time section (on top). b) Output when convolving with the Gaussian from @fig-seq_filtered_kernel(a). c) Output of the convolution with @fig-seq_filtered_kernel(b), and d) output of the convolution with @fig-seq_filtered_kernel(c).](figures/temporal_filters/seq_filtered_blur.png){#fig-sec_filtered_blur}

### Temporal derivatives

Spatial derivatives were useful to find regions of image variation such as object boundaries. Temporal derivatives can be used to locate moving objects. We can approximate a temporal derivative for discrete signals as:
$$
f \left[m,n,t\right] - f \left[m,n,t-1\right]
$$

As in the spatial case, it is useful to compute temporal derivatives of spatio-temporal Gaussians:
$$
\frac{\partial g}{\partial t} = \frac{-t}{\sigma_t^2} g(x,y,t)
$$
where $g(x,y,t)$ is the Gaussian as written in @eq-gauss3dcont. We can compute the spatio-temporal gradient of a Gaussian:
$$
\nabla  g = \left( g_x(x,y,t), g_y(x,y,t), g_t(x,y,t) \right) =  \left(-x/\sigma^2, -y/\sigma^2, -t/\sigma_t^2 \right) g(x,y,t)
$$

What should we do if we want to remove only the objects moving at a particular velocity?

In the case of a moving image with velocity $(v_x, v_y)$, the sequence is $f (x,y,t) = f_0 (x-v_xt,y-v_yt)$, we can compute the temporal derivative of $f(x,y,t)$ as:
$$
\frac{\partial f}{\partial t} = \frac{\partial f_0}{\partial t} = -v_x \frac{\partial f_0}{\partial x} - v_y \frac{\partial f_0}{\partial y}
\label{eq-brightnessconstancy}
$$

![Visualization of the space-time Gaussian. The Gaussian has a width of $\sigma^2=\sigma_t^2=1.5$, and has been discretized as a 3D array of size $7 \times 7 \times 7$. Each image shows one frame. a) Gaussian b) The partial derivative of the Gaussian with respect to $t$. c) Derivative along $v=(1,0)$ pixels/frame. d) $v=(-1,0)$ pixels/frame.](figures/temporal_filters/gaussians_xyt_seq.png){#fig-gaussian_seq}

![Spatio-temporal Gaussian $g\left[ n,t \right]$ and derivatives. a) Gaussian with $\sigma^2=1.5$. b) Partial derivative with respect to $t$. c) Partial derivative along $(1,-1)$, d) Partial derivative along $(1,1)$.](figures/temporal_filters/gaussians_xyt_section.png){#fig-gaussian_xyt_section}

If we compute the gradient of the Gaussian along the vector $\left[1,v_x,v_y\right]$:
$$
h(x,y,t;v_x,v_y) = g_t+v_xg_x+v_yg_y = \nabla  g \left( 1,v_x,v_y \right)^T
$$
and we convolve it with $f_0 (x-v_xt,y-v_yt)$ we get a zero output (using \@eq-brightnessconstancy):
$$
f_0 (x-v_xt,y-v_yt) \circ h = f_0 (x-v_xt,y-v_yt) \circ \left(  g_t+v_xg_x+v_yg_y \right) = \left( \frac{\partial f_0}{\partial t} + v_x \frac{\partial f_0}{\partial x} + v_y \frac{\partial f_0}{\partial y} \right) \circ g = 0
$$
The filter $h$ is shown in @fig-gaussian_seq as a sequence for different velocities. In the example shown in the figure, the Gaussian (@fig-gaussian_seq(a)) has a width of $\sigma^2=\sigma_t^2=1.5$, and has been discretized as a 3D array of size $7 \times 7 \times 7$. Figures b, c, and d show the filter $h$ for different velocities: $(v_x,v_y) =$ $(0,0)$, $(1,0)$, and $(-1,0)$. @fig-gaussian_xyt_section shows the corresponding space-time sections of the same spatio-temporal Gaussian derivatives. Both visualizations are equivalent and help to understand how the filter works.

Such a filter will cancel any objects moving at the velocity $(v_x,v_y)$. By using different filters, each one computing derivatives along different space-time orientations, we can create a set of filters that each respond to specific motion directions.

![a) Input sequence. b) $v_x=v_y=0$, c) $v_x=1$ pixels/frame. d) $v_x=-1$ pixel/frame.](figures/temporal_filters/seq_filtered_der.png){#fig-tunedfilter}

### Spatiotemporal Gabor filters

Just as we did with Gaussian derivatives, extending Gabor filters for motion analysis is a direct generalization of the x-y 2D Gabor function to an x-y-t 3D Gabor function. @fig-spacetimefilts(a) shows an x-t (cosine and sine) Gabor function in one spatial dimension, and @fig-spacetimefilts(b) shows a sketch of its Fourier transform. This function is selective to signals translating to the right with a speed $v=1$, i.e., $f(x-t)$. The red line in @fig-spacetimefilts(b) shows the Dirac line that contains the energy of the moving signal.

In 2 spatial dimensions, @fig-spacetimefilts(c) shows the sketch of the Gabor transfer function. Note that the x-y-t Gabor filter is not selective to velocity. If we have a 2D moving signal $f(x-v_xt, y-v_yt)$, the Fourier transform is contained inside a Dirac plane. Therefore, there are an infinite number of planes that will pass by the frequencies of the Gabor filter. All those planes intersect the red line shown in @fig-spacetimefilts(c). A single Gabor filter cannot disambiguate the input velocity.

![Space-time Gabor filters. a) Cosine and sine $x$-$t$ Gabor filter, and b) the sketch of its transfer function. c) Sketch of the transfer function of a spatiotemporal Gabor filter in 2 spatial dimensions ($x$-$y$-$t$).](figures/temporal_filters/gabor_spacetime_FT.png){#fig-spacetimefilts}

## Velocity-tuned filters {#sect-velocityTunedFilters}

How can we measure input velocity? There are many different approaches in the computer vision community for measuring motion. Here we show that it is possible to measure motion even with the simple processing machinery that we've developed so far.

We can use quadrature pairs of oriented filters in space-time to find motion speed and direction in the video signal. We just need to find the space-time orientation of the strongest response. @fig-spacetimetiles(a) shows a set of Gabor filters sampling the space-time frequency domain. When the input contains a moving signal, we can use a set of filters to identify the plane in the Fourier domain that contains the input energy. @fig-spacetimetiles(b) shows the subset of filters that have the strongest output for a particular input motion.

As an illustration, @fig-MT_velocity_tuned shows one possible architecture to create velocity-selective units. The first layer is composed of Gabor filters (cosine and sine) which are frequency-selective units. The outputs are combined according to different planes in the Fourier domain to create velocity-selective outputs.

Given an input sequence, one can estimate velocity by looking at the velocity-tuned unit with the strongest response.

![a) Space-time Gabor filters tiles. b) Set of Gabor filters selective to a particular velocity.](figures/temporal_filters/gabor_spacetime_tiles.png){#fig-spacetimetiles}

![Architecture to create velocity-selective units. The first layer is composed by space-time Gabor filters (cosine and sine) which are frequency-selective units. Here we represent the impulse response of each filter by a small x-y-t cube. For each quadrature pair we compute the amplitude. Then amplitude outputs are combined according to form different planes in the Fourier domain to create velocity-selective outputs. A normalization layer can be added to normalize the outputs by dividing every output by the sum of all the amplitudes (not shown). The full architecture is non-linear.](figures/temporal_filters/MT_velocity_tuned.png){#fig-MT_velocity_tuned}

## Human Motion Processing

While researchers aren't sure of the precise computations involved in human motion processing, some experiments can distinguish between classes of algorithms that the visual system may use. The methods of the previous section are based on **spatio-temporal filtering** [@Adelson85]. A second class of motion algorithms includes correlation-based methods and can be called **pattern matching methods** [@Adelson85]. In such methods, the motion offset of a spatial pattern from some starting position is computed by finding the position of highest correlation with the spatial pattern.

Adelson and Bergen proposed a beautiful motion illusion that distinguishes between two classes of motion algorithms that might be used by the visual system. The illusion involves temporal filtering, motion processing, and aliasing and so provides a good review of the material in this chapter.

The illusion is presented in the video in @fig-blends. The signals, and magnitudes of their space-time Fourier transforms, are developed in @fig-motionIllusion1 and @fig-motionIllusion2, building-up from simpler signals. The three rows of @fig-motionIllusion1 show a stationary sinusoid, a moving sinusoid, and a moving square wave. The spatio-temporal Fourier transform of the stationary sinusoid, $I(x) = \cos(\pi \omega x)$, is $\delta(\nu) (\delta(\omega) + \delta(-\omega))$. We have added a constant bias to the sinusoid to avoid negative intensity values, leading to an impulse at the center of the Fourier transform figure, @fig-motionIllusion1(c). The resulting 3 collinear impulses are along the temporal frequency, $\nu = 0$ line. A space-time plot of the signal, @fig-motionIllusion1(b), shows only vertical structures, indicating no motion.

A moving sinusoid has a similar Fourier transform magnitude, @fig-motionIllusion1(f), but with the spatio-temporal energies along a line perpendicular to the moving structures in the spatio-temporal signal, @fig-motionIllusion1(e). A moving square wave is similar, but the extra harmonics needed to construct the square wave are visible in the Fourier transform, @fig-motionIllusion1(i).

Continuing the development of the illusion, @fig-motionIllusion2(c) shows the Fourier transform and space-time plot of a square wave moving in 1/4 period jumps each time increment. This signal can be formed from @fig-motionIllusion1(h) by applying a periodic sample-and-hold function, resulting in the spectrum of @fig-motionIllusion1(i) replicated over temporal frequencies and multiplied by a sinc function over temporal frequency. The resulting Fourier transform magnitude is shown in @fig-motionIllusion2(c).

Because orientation in space-time tells motion direction, \@sect-modelingSequences, the space-time plot of @fig-motionIllusion2(b) shows that the motion should be perceived to the left. This will be consistent with the behavior of velocity-tuned filters, \@sect-velocityTunedFilters, responding to the lowest spatio-temporal frequency impulses shown in @fig-motionIllusion2(c). However, if we remove the lowest spatial frequency sinusoid from the signal, the result is shown in @fig-motionIllusion2(e), with the spatio-temporal Fourier transform @fig-motionIllusion2(g). Now the lowest spatio-temporal frequency cosine wave is oriented in the other direction. This opposite slope is also visible in the spatial domain, in the space-time plot of @fig-motionIllusion2(e), and especially if we low-pass filter that, resulting in @fig-motionIllusion2(f).

The signal of the second row of @fig-motionIllusion2 poses a conundrum. It can be argued that the signal moves to the left, just as the signal of row 1 of @fig-motionIllusion2 does. The pattern match—the minimum correlation signal—indeed moves to the left. But a vision system examining the orientation of the lowest spatio-temporal frequency components of the signal in @fig-motionIllusion2(g), or looking at the dominant orientations in the space-time plots of @fig-motionIllusion2(e) and (g), would find a signal moving to the right.

How does the signal appear to move to you? We invite you to play the videos of @fig-motionIllusion2(c) and (g) (set your player to loop the videos) to assess which way each of the signals of @fig-motionIllusion2 moves. @fig-blends shows slow and fast motion versions of a blended signal, where the top half contains the lowest sinusoidal spatial frequency component of the square wave and the bottom half does not. By moving your eye vertically, you can convince yourself that the entire pattern is moving rigidly to the left, yet it also appears that the top half is moving to the left and the bottom half is moving to the right. This demonstration gives evidence for spatio-temporal filter-based motion processing within the human visual system, since such filtering would predict leftward motion for the top halves of the videos in @fig-blends, and rightward motion for the bottom halves of those videos.

![Space-time signals, building toward the fluted square-wave motion illusion. First row: stationary sine wave, (a) Movie of a motionless sine wave. (b) The space-time plot shows only vertical structure. (c) spatio-temporal Fourier transform has all energy on the zero temporal frequency axis, since nothing is moving. Second row: (d) moving sine wave. (e) In the space-time plot, speed corresponds to local orientation. (f) The Fourier transform energy is sheared according to the sine wave's speed. Third row: Moving square wave. The additional harmonics required to form a square wave are visible in the spatio-temporal Fourier transform, (i).](figures/temporal_filters/motionIllusion1.png){#fig-motionIllusion1}

![Derivation of the fluted square-wave motion illusion, continued from @fig-motionIllusion1. First row: This square wave moves in 1/4 wavelength jumps, instead of continuously. This staggered motion generates the additional spatio-temporal frequencies shown in (a). The lowest spatio-temporal frequency still indicates motion to the left. Second row: If we remove the lowest spatial frequency sine wave of the square wave, creating a "fluted square wave," then the lowest spatio-temporal frequency now moves in the other direction. This is also visible from the space time plot in (e), and especially in the spatio-temporally low-pass filtered version, (f).](figures/temporal_filters/motionIllusion2.png){#fig-motionIllusion2}

![Impossible combination of ordinary and fluted square-waves (view on looping display). While one can verify, by tracing a finger or scanning your eyes, that the entire structure is moving rigidly to the left, the bottom half appears to be moving to the right. At a fast speed, (b), the effect is accentuated.](figures/temporal_filters/blends.png){#fig-blends}

## Concluding remarks
