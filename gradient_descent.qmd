# Gradient-Based Learning Algorithms {#chapter-gradient_descent}

## Introduction

Once you have specified a learning problem (loss function, hypothesis
space, parameterization), the next step is to find the parameters that
minimize the loss. This is an optimization problem, and the most common
optimization algorithm we will use is **gradient descent**. Gradient
descent is like a skier making their way down a snowy mountain, where
the shape of the mountain is the loss function.

There are many varieties of gradient descent, and we will call this
whole family **gradient-based learning algorithms**. All share the same
basic idea: at some operating point, calculate the direction of steepest
descent, then use this direction to find a new operating point with
lower loss.

## Technical Setting

In this chapter, we consider the task of minimizing a cost function
$J: \cdot \rightarrow \mathbb{R}$, which is a function that maps some
arbitrary input to a scalar cost.

In learning problems, the domain of $J$ is the training data and the
parameters $\theta$. Often, we will consider the training data to be
fixed and only denote the objective as a function of the parameters,
$J(\theta)$. Our goal is to solve: $$\begin{aligned}
    \theta^* = \mathop{\mathrm{arg\,min}}_{\theta} J(\theta)
\end{aligned}$$

Pretty much all optimizers work by some iterative process, where they
update the parameters to be better and better. Different optimizers
differ in how the parameter update function works. The update function
gets to view some information about the loss landscape, then uses that
information to update the parameters, as shown in
@fig-gradient_descent:optimization_schematic.

![](./figures/gradient_descent/optimization_schematic.png){width="35%"}

In the simplest setting, called **zeroth-order optimization**, the
update function only gets to observe the value $J(\theta)$. The only
way, then, to find $\theta$'s that minimize the loss is to sample
different values for $\theta$ and move toward the values that are lower.

For gradient-based optimization, also called **first-order
optimization**, the update function takes as input the gradient of the
cost with respect to the parameters at the current operating point,
$\nabla_{\theta}J(\theta)$. This reveals hugely useful information about
the loss that directly tells us how to minimize it: just move in the
direction of steepest descent, that is, the gradient direction.

Higher-order optimization methods observe higher-order derivatives of
the loss, such as the Hessian $H$, which tells you how the landscape is
locally curving. The Hessian is costly to compute but many methods use
approximations to the Hessian, or other properties related to loss
curvature, and these are growing in
popularity @martens2015optimizing, @foret2020sharpness.

## Basic Gradient Descent

The simplest version of gradient descent just takes a step in the
gradient direction of length proportional to the gradient magnitude.
This algorithm is described in algorithm
[\[alg:gradient_descent:basic_gradient_descent\]](#alg:gradient_descent:basic_gradient_descent){reference-type="ref"
reference="alg:gradient_descent:basic_gradient_descent"}.

::: algorithm
[]{#alg:gradient_descent:basic_gradient_descent
label="alg:gradient_descent:basic_gradient_descent"} **Input:**
objective function $J$, initial parameter vector $\theta^0$, learning
rate $\eta$, number of steps $K$ **Output:** trained parameter vector
$\theta^* = \theta^K$
:::

This algorithm has two hyperparameters, the **learning rate** $\eta$,
which controls the step size (learning rate times gradient magnitude),
and the number of steps $K$. If the learning rate is sufficiently small
and the initial parameter vector $\theta^0$ is random, then this
algorithm will almost surely converge to a local minimum of $J$ as
$K \rightarrow \infty$ @lee2016gradient. However, to descend more
quickly, it can be useful to set the learning rate to a higher value.

## Learning Rate Schedules

A generally useful strategy is to start with a high value for $\eta$ and
then decay it until convergence according to a **learning rate
schedule**. Researchers have come up with innumerable schedules and they
generally work by calling some function $\texttt{lr}(\eta^0,k)$ to get
the learning rate on each iteration of descent: $$\begin{aligned}
    \eta^{k} = \texttt{lr}(\eta^0,k)
\end{aligned}$$ Generally, we want an update rule where
$\eta^{k+1} < \eta^k$, so that we take smaller steps as we approach the
minimizer. A few simple and popular approaches are given below:
$$\begin{aligned}
    \texttt{lr}(\eta^0,k) &= \beta^{-k} \eta^0 &\quad\quad \triangleleft\quad \text{exponential decay}\\
    \texttt{lr}(\eta^0,k) &= \beta^{-\lfloor k/M \rfloor} \eta^0 &\quad\quad \triangleleft\quad \text{stepwise exponential decay}\\
    \texttt{lr}(\eta^0,k) &= \frac{(K - k)}{K} \eta^0 &\quad\quad \triangleleft\quad \text{linear decay}
\end{aligned}$$  The $\beta$ and $M$ are additional
hyperparameters of these methods. The general approach of learning rate
decay is summarized in algorithm
[\[alg:gradient_descent:gradient_descent_with_lr_decay\]](#alg:gradient_descent:gradient_descent_with_lr_decay){reference-type="ref"
reference="alg:gradient_descent:gradient_descent_with_lr_decay"}.

::: algorithm
[]{#alg:gradient_descent:gradient_descent_with_lr_decay
label="alg:gradient_descent:gradient_descent_with_lr_decay"} **Input:**
objective function $J$, initial parameter vector $\theta^0$, initial
learning rate $\eta^0$, learning rate function $\texttt{lr}$, number of
steps $K$ **Output:** trained parameter vector $\theta^* = \theta^K$
:::

Variations on this algorithm include only decaying the learning rate
when a plateau is reached (i.e., when the loss is not decreasing for
many iterations in a row), or decaying the learning rate according to
more complex nonlinear schedules, such as one shaped like a cosine
function @loshchilov2016sgdr.

## Momentum

Could we do a smarter update than just taking a step in the direction of
the gradient? Of the countless ideas that have been proposed, one of the
few that has stuck is
**momentum** @polyak1964some, @sutskever2013importance. Momentum makes
the analogy to skiing even more precise: momentum is like the inertia of
the skier, carrying them over the little bumps and imperfections in the
ski slope and increasing their speed as they descend along a straight
path. In math, momentum just means that we set the parameter update to
be a direction $\mathbf{v}^{k+1}$, given by a weighted combination of
the previous update direction, $\mathbf{v}^{k}$, plus the current
negative gradient: $$\begin{aligned}
    \mathbf{v}^{k+1} = \mu \mathbf{v}^{k} - \eta\nabla_{\theta} J(\theta^k)
\end{aligned}$$ The weight $\mu$ in this combination is a new
hyperparameter, sometimes simply called the momentum. The full algorithm
is given in algorithm
[\[alg:gradient_descent:gradient_descent_with_momentum\]](#alg:gradient_descent:gradient_descent_with_momentum){reference-type="ref"
reference="alg:gradient_descent:gradient_descent_with_momentum"}.

::: algorithm
[]{#alg:gradient_descent:gradient_descent_with_momentum
label="alg:gradient_descent:gradient_descent_with_momentum"} **Input:**
objective function $J$, initial parameter vector $\theta^0$, learning
rate $\eta$, momentum $\mu$, number of steps $K$ **Output:** trained
parameter vector $\theta^* = \theta^K$ $\mathbf{v}^{0} = \mathbf{0}$
:::

@fig-gradient_descent:momentum_out1 shows how momentum affects gradient
descent for a simple objective $J = \texttt{abs}(\theta)$ (absolute
value of $\theta$). As can be seen in the figure, some momentum can help
convergence rate (@fig-gradient_descent:momentum_out1, $\mu = 0.5$) but
too much momentum will cause the trajectory to overshoot the optimum and
even when the optimum loss is achieved, the trajectory might not stop
(@fig-gradient_descent:momentum_out1, $\mu = 0.95$).

![](./figures/gradient_descent/momentum_out1.png){width="100%"}

It is also possible to come up with other kinds of momentum, which bias
the update direction based on some accumulated information from previous
updates. Two popular alternatives, which you can read up on elsewhere,
are Nesterov's accelerated gradient @nesterov1983method and
Adam @kingma2014adam.

## What Kinds of Functions Can Be Minimized with Gradient Descent?

What if a function is not differentiable? Can we still use gradient
descent? Sometimes! The property we need is that we can get a meaningful
signal as to how to perturb the function's parameters in order to reduce
the loss. This property is *not* the same as differentiability defined
in math textbooks. A function may be differentiable but not give useful
gradients (e.g., if the gradient is zero everywhere), and a function may
be nondifferentiable (at certain points) but still allow for meaningful
gradient-based updates (e.g., `abs`).

@fig-gradient_descent:grad_descent_simple_examples gives examples of
different types of functions being minimized with gradient descent. (b)
and
[\[fig:gradient_descent:grad_descent_simple_examples\]](#fig-gradient_descent:grad_descent_simple_examples){reference-type="ref"
reference="fig:gradient_descent:grad_descent_simple_examples"}(d) are
cases where the function is discontinuous, and the analytical derivative
is undefined at the discontinuity. Surprisingly, in
@fig-gradient_descent:grad_descent_simple_examples(b), this is not a
problem for gradient descent. This is because the gradient descent
algorithm we are using here (the one used by Pytorch @paszke2019pytorch)
uses a **one-sided derivative** at the discontinuity, that is, we set
the gradient at the discontinuity to be equal to the gradient value an
infinitesimal step away from the discontinuouity in a fixed arbitrary
direction. Under the hood, for each atomic discontinuous operation,
Pytorch requires that we define its gradients at the discontinuouities,
and the one-sided gradient is a standard choice. This is why it can be
fine in deep learning (chapter
[\[chapter-neural_nets\]](#sec-neural_nets){reference-type="ref"
reference="chapter-neural_nets"}) to use functions like rectified
nonlinear units (relus), which are common in deep networks and have
discontinuous gradients.

\(c\) and (e) give cases where the function is continuous, but the
gradients are not well-behaved. In
@fig-gradient_descent:grad_descent_simple_examples(c) we have a
gradient that has nearly **vanished**, that is, it is near zero
everywhere and gradient descent, with a fixed learning rate, will
therefore be slow.
@fig-gradient_descent:grad_descent_simple_examples(e) shows the
opposite scenario: the gradient at the minimizer goes to infinity; we
call this an **exploding gradient**, and this leads to failures of
convergence.

Finally, @fig-gradient_descent:grad_descent_simple_examples(f) shows
one more problematic case: when there are multiple minima, gradient
descent can get stuck in a suboptimal minimum. Which minimum we arrive
at will depend on where we initialized $x$.

  ------------------------------------------------------------------------ ------------------------------------------------------------------------
                                \(a\) convex                                                         \(b\) discontinuous
   ![](./figures/gradient_descent/grad_descent_ex1.png){width="50%"}   ![](./figures/gradient_descent/grad_descent_ex2.png){width="50%"}
                          \(c\) vanishing gradient                                           \(d\) zero gradient + discontinuous
   ![](./figures/gradient_descent/grad_descent_ex4.png){width="50%"}   ![](./figures/gradient_descent/grad_descent_ex3.png){width="50%"}
                          \(e\) exploding gradient                                               \(f\) multiple local minima
   ![](./figures/gradient_descent/grad_descent_ex5.png){width="50%"}   ![](./figures/gradient_descent/grad_descent_ex6.png){width="50%"}
  ------------------------------------------------------------------------ ------------------------------------------------------------------------

### Gradient-Like Optimization for Functions without Good Gradients {#sec-gradient_descent:zeroth_order}

What about minimizing functions like
@fig-gradient_descent:grad_descent_simple_examples(d), where the
gradient is zero almost everywhere? This is a case where gradient
descent truly struggles. However, it is often possible to transform such
a problem into one that can be treated with gradient descent. Remember
that the key property of a gradient, from the perspective of
optimization, is that it is a locally loss-minimizing direction in
parameter space. Most gradient-based optimizers don't really need true
gradients; instead their update functions are compatible with a broader
family of local loss-minimizing directions, $\mathbf{v}$.

Besides the true gradient, what are some other good choices for
$\mathbf{v}$? One common idea is to set $\mathbf{v}$ to be the gradient
of a **surrogate loss** function, which is a function,
$J_{\texttt{surr}}$, with meaningful (non-zero) gradients, that
approximates $J$. An example might be a smoothed version of $J$. Another
way to get $\mathbf{v}$ is to compute it by sampling perturbations of
$\theta$, and seeing which perturbation leads to lower loss. In this
strategy, we evaluate $J(\theta+\epsilon)$ for a set of perturbations
$\epsilon$, then move toward the $\epsilon$'s that decreased the loss.
Approaches of this kind are sometimes called **evolution
strategies** @beyer2002evolution, @salimans2017evolution, and a basic
version of this algorithm is given in algorithm
[\[alg:gradient_descent:ES\]](#alg:gradient_descent:ES){reference-type="ref"
reference="alg:gradient_descent:ES"}.

::: algorithm
[]{#alg:gradient_descent:ES label="alg:gradient_descent:ES"} **Input:**
objective function $J$, initial parameter vector $\theta^0$, learning
rate $\eta$, sampling standard deviation $\sigma$, number of samples
$M$, number of steps $K$ **Output:** trained parameter vector
$\theta^* = \theta^K$
:::

As shown in @fig-gradient_descent:sampling_out1, this algorithm can
successfully minimize the function in
@fig-gradient_descent:grad_descent_simple_examples(c).

![](./figures/gradient_descent/sampling_out1.png){width="50%"}

### Gradient Clipping

What about @fig-gradient_descent:grad_descent_simple_examples(e), where
the gradient explodes near the optimum? Is there anything we can do to
improve optimization of this function? To combat exploding gradients, a
useful trick is **gradient clipping**, which just means clamping the
magnitude of the gradient to some maximum value. Algorithm
[\[alg:gradient_descent:grad_clipping\]](#alg:gradient_descent:grad_clipping){reference-type="ref"
reference="alg:gradient_descent:grad_clipping"} describes this approach.

::: algorithm
[]{#alg:gradient_descent:grad_clipping
label="alg:gradient_descent:grad_clipping"} **Input:** objective
function $J$, initial parameter vector $\theta^0$, learning rate $\eta$,
number of steps $K$, max gradient magnitude $m$ **Output:** trained
parameter vector $\theta^* = \theta^K$
:::



This algorithm indeed successfully minimizes our example of the
exploding gradient, as can be seen in
@fig-gradient_descent:clipped_out1.

![](./figures/gradient_descent/clipped_out1.png){width="50%"}

## Stochastic Gradient Descent {#sec-gradient_descent:SGD}

One problem with the gradient-based methods we have seen so far is that
the gradient may in fact be very expensive to compute, and this is often
the case for learning problems. This is because learning problems
typically have the form that $J$ is the average of losses incurred on
each training datapoint. Computing $\nabla_{\theta}J(\theta)$ requires
computing the gradient for each element in the average, that is, the
gradient of the function being learned evaluated at the location of each
datapoint in the training set. If we train on a big dataset, say 1
million training points, then to perform just *one* step of gradient
descent requires computing 1 million gradients! To make this clear, we
will write out $J$ as an explicit function of the training data
$\{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}\}_{i=1}^N$. For typical learning
problems,
$\nabla_{\theta} J(\theta, \{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}\}_{i=1}^N)$
decomposes as follows: $$\begin{aligned}
    \nabla_{\theta} J(\theta, \{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}\}_{i=1}^N) &= %\frac{1}{N}\sum_{i=1}^N \nabla_{\theta} \mathcal{J}(\theta, \mathbf{x}^{(i)}, \mathbf{y}^{(i)})\\
    \nabla_{\theta} \frac{1}{N}\sum_{i=1}^N \mathcal{L}(f_{\theta}(\mathbf{x}^{(i)}), \mathbf{y}^{(i)})\\
    &= \frac{1}{N}\sum_{i=1}^N \nabla_{\theta} \mathcal{L}(f_{\theta}(\mathbf{x}^{(i)}), \mathbf{y}^{(i)})
\end{aligned}$$

For large $N$, computing this sum is very expensive. Suppose instead we
randomly subsample (without replacement) a *batch* of terms from this
sum, $\{\mathbf{x}^{(b)}, \mathbf{y}^{(b)}\}_{b=1}^B$, where $B$ is the
**batch size**. We then compute an *estimate* of the total gradient as
the average gradient over this batch as follows: $$\begin{aligned}
    \tilde{\mathbf{g}} = \frac{1}{N}\sum_{b=1}^B \nabla_{\theta} \mathcal{L}(f_{\theta}(\mathbf{x}^{(b)}), \mathbf{y}^{(b)})
\end{aligned}$$ If we sample a large batch, where $B$ is almost as large
as $N$, then the average over the $B$ terms should be roughly the same
as the average over all $N$ terms. If we sample a smaller batch, then
our estimate of the gradient will be less accurate but faster to
compute. Therefore we have a tradeoff between accuracy and speed, and we
can navigate this tradeoff with the hyperparameter $B$. The variant of
gradient descent that uses this idea is called **stochastic gradient
descent** (SGD), because each iteration of descent uses a different
randomly (stochastically) sampled batch of training data to estimate the
gradient. The full description of `SGD` is given in algorithm
[\[alg:gradient_descent:SGD\]](#alg:gradient_descent:SGD){reference-type="ref"
reference="alg:gradient_descent:SGD"}.

::: algorithm
[]{#alg:gradient_descent:SGD label="alg:gradient_descent:SGD"}
**Input:** initial parameter vector $\theta^{0}$, data
$\{\mathbf{x}^{(i)},\mathbf{y}^{(i)}\}_{i=1}^N$, learning rate $\eta$,
batch size $B$, number of steps $K$ **Output:** trained parameter vector
$\theta^* = \theta^K$
:::

`SGD` has a number of useful properties beyond just being faster to
compute than `GD`. Because each step of descent is somewhat random,
`SGD` can jump over small bumps in the loss landscape, as long those
bumps disappear for some randomly sampled batches. Another important
property is that `SGD` can implicitly regularize the learning problem.
For example, for linear problems (i.e., $f_\theta$ is linear), then if
there are multiple parameter settings that minimize the loss, `SGD` will
often converge to the solution with minimum parameter
norm @zhang2021understanding.

## Concluding Remarks

The study of optimization can fill dozens of textbooks and thousands of
academic papers. But fortunately for us, modern machine learning has
converged on just a few very simple optimization methods that are used
in practice. We will soon encounter deep learning, which is the main
kind of machine learning used for computer vision. In deep learning,
gradient-based optimization is the workhorse. Believe it or not, the
handful of algorithms described above are enough to train most
state-of-the-art deep learning models. Every year there are new
elaborations on these ideas, and second-order methods are ever on the
horizon, yet the basic concepts remain quite simple: compute a local
estimate of the shape of the loss landscape, then, based on this shape,
take a small step toward a lower loss.
