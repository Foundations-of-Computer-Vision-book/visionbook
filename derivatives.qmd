
# Image Derivatives {#sec-image_derivatives}

## Introduction

Computing image derivatives is an essential operator for extracting useful information from images. As we show in the previous chapter, image derivatives allowed us to compute boundaries between objects and to have access to some of the three-dimensional (3D) information lost when projecting the 3D world into the camera plane. Derivatives are useful because they give us information about where changes are happening in the image, and we expect those changes to be correlated with transitions between objects.

The operator that computes the image derivatives along the spatial dimensions ($x$ and $y$) is a linear system:

![Computing image derivatives along $x$- and $y$-dimensions.](figures/derivatives/genericfilterH.png){#fig-genericfilterH}

The derivative operator is linear and translation invariant. Therefore, it can be written as a convolution. The following images show an input image and the resulting derivatives along the horizontal and vertical dimensions using one of the discrete approximations that we will discuss in this chapter.

![Image derivatives. (left) Input image, (middle) its $x$-derivative, and (right) its $y$-derivative.](figures/derivatives/mit_der_c.jpg){#fig-derivativesmit}

## Discretizing Image Derivatives

If we had access to the continuous image, then image derivatives could be computed as: $\partial \img (x,y) / \partial x$, which is defined as:

$$
\frac{\partial \img(x,y)} {\partial x} = \lim_{\epsilon \to 0} \frac{ \img(x+\epsilon,y) -\img(x,y)} {\epsilon}
$$

However, there are several reasons why we might not be able to apply this definition:

- We only have access to a sampled version of the input image, $\img \left[n,m\right]$, and we cannot compute the limit when $\epsilon$ goes to zero.
- The image could contain many non-derivable points, and the gradient would not be defined. We will see how to address this issue later when we study Gaussian derivatives.
- In the presence of noise, the image derivative might not be meaningful as it might just be dominated by the noise and not by the image content.

For now, let's focus on the problem of approximating the continuous derivative with discrete operators. As the derivative is a linear operator, it can be approximated by a discrete linear filter. There are several ways in which image derivatives can be approximated.

Let's start with a simple approximation to the derivative operator that we have already played with $d_0  = \left[1, -1 \right]$. In one dimension (1D), convolving a signal $\img \left[n \right]$ with this filter results in:
$$
\img \circ d_0 = \img \left[n \right] - \img \left[n-1 \right]
$$
This approximates the derivative by the difference between consecutive values, which is obtained when $\epsilon=1$. @fig-discretederivative (c) shows the result of filtering a 1D signal (@fig-discretederivative [a]) convolved with $d_0 \left[n\right]$ (@fig-discretederivative [b]). The output is zero wherever the input signal is constant, and it is large in the places where there are variations in the input values. However, note that the output is not perfectly aligned with the input. In fact, there is half a sample displacement to the right. This is due to the fact that $d_0 \left[n\right]$ is not centered around the origin.

This can be addressed with a different approximation to the spatial derivative $d_1  = \left[1, 0, -1 \right]/2$. In one dimension, convolving a signal $\img \left[n \right]$ with $d_1 \left[n\right]$ results in:
$$
\img \circ d_1 = \frac{\img \left[n+1 \right] - \img \left[n-1 \right]}{2}
$$
@fig-discretederivative (e) shows the result of filtering the 1D signal (@fig-discretederivative [a]) convolved with $d_1 \left[n\right]$ (@fig-discretederivative [d]). Now the output shows the highest magnitude output in the midpoint where there is variation in the input signal.

![(a) Input signal, $\img [n]$. (b) Convolutional kernel $d_0 [n]$, defined as $d_0 [0]=1$ and $d_0 [1]=-1$ and zero everywhere else. (c) Output of the convolution between $\img [n]$ and $d_0 [n]$. (d) Kernel $d_1 [n]$, defined as $d_1 [-1]=1$ and $d_1 [1]=-1$ and zero everywhere else. (e) Output of the convolution between $\img [n]$ and $d_1 [n]$.](figures/derivatives/discretederivative.png){#fig-discretederivative}

It is also interesting to see the behavior of the derivative and its discrete approximations in the Fourier domain. In the continuous domain, the relationship between the Fourier transform on a function and the Fourier transform of its derivative is:
$$
\frac{\partial \img (x)}{\partial x}  
\xrightarrow{\mathscr{F}} 
j w \capitalimg (w)
$$
In the continuous Fourier domain, derivation is equivalent to multiplying by $jw$. In the discrete domain, the DFT of a signal derivative will depend on the approximation used. Let's study now the DFT of the two approximations that we have discussed here, $d_0$ and $d_1$.

The DFT of $d_0 \left[n\right]$ is:
$$
\begin{split}
D_0 \left[u \right] & = 1 - \exp \left( -2 \pi j \frac{u}{N} \right)  \\
& = \exp \left( - \pi j \frac{u}{N} \right) \left(  \exp \left( \pi j \frac{u}{N} \right) - \exp \left( -\pi j \frac{u}{N} \right)  \right) \\
& = \exp \left( - \pi j \frac{u}{N} \right) 2 j \sin (\pi u /N)
\end{split}
$$
The first term is a pure phase shift, and it is responsible for the half a sample delay in the output. The second term is the amplitude gain, and it can be approximated by a linear dependency on $u$ for small $u$ values.

The DFT of $d_1 \left[n\right]$ is:
$$
\begin{split}
D_1 \left[u \right] & =  1/2\exp \left( 2 \pi j \frac{u}{N} \right) - 1/2 \exp \left( -2 \pi j \frac{u}{N} \right)  \\
& =  j \sin (2 \pi u /N)
\end{split}
$$

@fig-d0andd1_dft shows the magnitude of $D_0\left[u \right]$ and $D_1\left[u \right]$ and compares it with $\left| 2 \pi u/N \right|$, which will be the ideal approximation to the derivative. The amplitude of $D_0\left[u \right]$ provides a better approximation to the ideal derivative, but the phase of $D_0\left[u \right]$ introduces a small shift in the output. Conversely, $D_1\left[u \right]$ has no shift, but it approximates the derivative over a smaller range of frequencies. The output to $D_1\left[u \right]$ is smoother than the output to $D_0\left[u \right]$, and, in particular, $D_1\left[u \right]$ gives a zero output when the input is the signal $\left[ 1, -1, 1, -1, ... \right]$. In fact, we can see that $\left[1,0,-1\right] = \left[1,-1\right] \circ \left[1,1\right]$, and, therefore $D_1\left[u \right] = D_0\left[u \right] B_1\left[u \right]$, where $B_1\left[u \right]$ is the DFT of the binomial filter $b_1 \left[n \right]$.

![Magnitude of (a) $D_0\left[u \right]$ and (b) $D_1\left[u \right]$ and comparison with $\left| 2 \pi u/N \right|$, shown as a thin black line. Both DFTs are computed over 20 samples.](figures/derivatives/d0andd1_dft.png){#fig-d0andd1_dft}

When working with two-dimensional (2D) images, there are several ways in which partial image derivatives can be approximated. For instance, we can compute derivatives along

 the $n$ and $m$ components:

$$
\begin{bmatrix}
  1 \\
  -1
\end{bmatrix}
~~~~~~~
\begin{bmatrix}
  1 & -1
\end{bmatrix}
$$
The problem with this discretization of the image derivatives is that the outputs are spatially misaligned because a filter of length 2 shifts the output image by half a pixel as we discussed earlier. The spatial misalignment can be removed by using a slightly different version of the same operator: we can use a rotated reference frame as it is done in the **Roberts cross** operator, introduced in 1963 @cite(Roberts63) in a time when reading an image of $256 \times 256$ pixels into memory took several minutes:

$$
\begin{bmatrix}
  1 & ~0\\
  0 & -1
\end{bmatrix}
~~~~~~~
\begin{bmatrix}
  ~0 & 1 \\
  -1 & 0
\end{bmatrix}
$$
Now, both outputs are spatially aligned because they are shifted in the same way. Another discretization is the Sobel operator, based on the $[-1,0,1]$ kernel, and will be discussed in @sec-derivatives_binomial_filters.

While newer algorithms for edge extraction have been developed since the creation of these operators, they still hold value in cases where efficiency is important. These simple operators continue to be used as fundamental components in modern computer vision descriptors, including the Scale-invariant feature transform (SIFT) @cite(Lowe04) and the histogram of oriented gradients (HOG) @cite(Dalal2005).

## Gradient-Based Image Representation

Derivatives have become an important tool to represent images, and they can be used to extract a great deal of information from the image as it was shown in the previous chapter. One thing about derivatives is that it might seem as though we are losing information from the input image. An important question is if we have the derivative of a signal, can we recover the original image? What information is being lost? Intuitively, we should be able to recover the input image by integrating its derivative, but it is an interesting exercise to look in detail at how this integration can be performed. We will start with a 1D signal, and then we will discuss the 2D case.

A simple way of seeing that we can recover the input from its derivatives is to write the derivative in matrix form. This is the matrix that corresponds to the convolution with the kernel $\left[1, -1 \right]$ that we will call $\mathbf{D_0}$. The next two matrices show the matrix $\mathbf{D_0}$ and its inverse $\mathbf{D_0}^{-1}$ for a 1D image of length five pixels using zero boundary conditions:

$$
\mathbf{D_0} = 
\begin{bmatrix}
  1 ~& 0 ~& 0 ~& 0~& 0 \\
  -1 ~& 1 ~& 0 ~& 0~& 0 \\
  0 ~& -1 ~& 1 ~& 0 ~& 0\\
  0~& 0 ~& -1 ~& 1 ~& 0\\
  0~& 0 ~& 0 ~& -1 ~& 1
\end{bmatrix}
~~~~~~~~~
\mathbf{D_0}^{-1} = 
\begin{bmatrix}
  1 ~&~ 0 ~&~ 0 ~&~ 0~&~ 0 \\
  1 ~&~ 1 ~&~ 0 ~&~ 0~&~ 0 \\
  1 ~&~ 1 ~&~ 1 ~&~ 0 ~&~ 0\\
  1~&~ 1 ~&~ 1 ~&~ 1 ~&~ 0\\
  1~&~ 1 ~&~ 1 ~&~ 1 ~&~ 1
\end{bmatrix}
$$
We can see that the inverse $\mathbf{D_0}^{-1}$ is reconstructing each pixel as a sum of all the derivative values from the left-most pixel to the right. And the inverse perfectly reconstructs the input. But, this is cheating because the first sample of the derivative gets to see the actual value of the input signal, and then we can integrate back the entire signal. That matrix is assuming zero boundary conditions for the signal, and the boundary gives us the needed constraint to be able to integrate back the input signal.

But what happens if you only get to see valid differences and you remove any pixel that was affected by the boundary? In this case, the derivative operator in matrix form is:

$$
\mathbf{D_0} = 
\begin{bmatrix}
  -1 ~& 1 ~& 0 ~& 0~& 0 \\
  0 ~& -1 ~& 1 ~& 0 ~& 0\\
  0~& 0 ~& -1 ~& 1 ~& 0\\
  0~& 0 ~& 0 ~& -1 ~& 1
\end{bmatrix}
$$

Let's consider the next 1D input signal:

$$
\boldimg = \left[1, 1, 2, 2, 0\right]
$$

Then, the output of the derivative operator is:

$$
\mathbf{r}=\mathbf{D_0} \boldimg=\left[0, -1, 0, 2\right]
$$
Note that this vector has one sample less than the input. To recover the input $\boldimg$ we cannot invert $\mathbf{D_0}$ as it is not a square matrix, but we can compute the pseudoinverse, which turns out to be:

$$
\mathbf{D_0}^{+} = \frac{1}{5}
\begin{bmatrix}
  -4 ~& -3 ~& -2~& -1 \\
  1 ~& -3 ~& -2 ~&-1 \\
  1~& 2 ~& -2 ~& -1\\
  1~& 2 ~& 3 ~& -1\\
  1~& 2 ~& 3 ~& 4
\end{bmatrix}
$$
The pseudoinverse has an interesting structure, and it is easy to see how it can be written in the general form for signals of length $N$. Also, note that $\mathbf{D_0}^{+}$ cannot be written as a convolution. Another important thing is that the inversion process is trying to recover more samples than there are observations. The trade-off is that the signal that it will recover will have zero mean (so it loses one degree of freedom that cannot be estimated). In this example, the reconstructed input is:

$$
\hat{\boldimg} = \mathbf{D_0}^{+} \mathbf{r} =
\left[-0.2, -0.2, 0.8, 0.8, -1.2 \right]
$$
Note that $\hat{\boldimg}$ is a zero mean vector. In fact, the recovered input is a shifted version of the original input, $\hat{\boldimg} = \boldimg - 1.2$, where 1.2 is the mean value of samples on $\boldimg$. Then, you still can recover the input signal up to the DC component (mean signal value).

## Image Editing in the Gradient Domain {#sec-editinggradientdomain}

One of the interests of transforming an image into a different representation than pixels is that it allows us to manipulate aspects of the image that would be hard to control using the pixels directly. @fig-edit_with_derivatives shows an example of image editing using derivatives.

![Image inpainting: Using image derivatives, we delete the word “stop” by setting to zero the gradients indicated by the mask. The resulting decoded image propagates the red color inside the region that contained the word.](figures/derivatives/edit_with_derivatives.png){#fig-edit_with_derivatives}

First, the image is **encoded** using derivatives along the $x$ and $y$ dimensions:
$$
\mathbf{r} = 
\left[ 
\begin{array}{c}
\mathbf{D_x}  \\
\mathbf{D_y} 
\end{array}
\right] 
\boldimg 
$$
The resulting representation, $\mathbf{r}$, contains the concatenation of the output of both derivative operators. $\mathbf{r}$ will have high values in the image regions that contain changes in pixel intensities and colors and will be near zero in regions with small variations. This representation can be **decoded** back into the input image by using the pseudoinverse as we did in the 1D case. The pseudoinverse can be efficiently computed in the Fourier domain @cite(Weiss01derivingintrinsic).

We can now manipulate this representation. In the example shown in @fig-edit_with_derivatives, if we set to zero the image derivatives that correspond to the word “stop,” when we decode the representation, we obtain a new image where the word “stop” has been removed. The red color fills the region with the word present in the input image. We did not have to specify what color that region had to be; that information comes from the integration process. Therefore, deleting an object using image derivatives can be achieved by setting to zero the gradients regardless

 of the content that will fill that region. Similarly, we could remove any object in an image or create new textures by modifying its gradient representation.

## High-Order Gaussian Derivatives

The second-order derivative of a Gaussian is:
$$
g_{x^2}(x,y; \sigma) = \frac{x^2-\sigma^2}{\sigma^4} g(x,y; \sigma)
$$

The $n$-th order derivative of a Gaussian can be written as the product between a polynomial on $x$, with the same order as the derivative, times a Gaussian. The family of polynomials that result in computing Gaussian derivatives is called Hermite polynomials. The general expression for the $n$ derivative of a Gaussian is:
$$
g_{x^n}(x; \sigma) =  
\frac{\partial^{n} g(x)}{\partial x^n}
=
\left( \frac{-1}{\sigma \sqrt{2}} \right)^n
H_n\left( \frac{x}{\sigma \sqrt {2}} \right)
g(x; \sigma)
$$
The first Hermite polynomial is $H_0(x)=1$, the second is $H_1(x) = 2x$, the third is $H_2(x)=4x^2-2$, and they can be computed recursively as:
$$
H_n(x) = 2x H_{n-1}(x) - 2(n-1)H_{n-2}(x)
$$
The second-order Gaussian derivative with a $\sigma=1$ can be seen in @fig-Gaussian_derivatives2.

![Second derivative of a Gaussian with $\sigma=1$ in the horizontal direction.](figures/derivatives/derivative_gaussian.png){#fig-Gaussian_derivatives2}

When implementing a linear system to compute the derivative of an image, we would be convolving the image with the derivative of the Gaussian. However, computing a high-order Gaussian derivative as a single step convolution might not be the best solution, and, in practice, we prefer to compute it in a step-by-step fashion.

## Derivatives using Binomial Filters {#sec-derivatives_binomial_filters}

One of the main problems of computing image derivatives is that the operation will enhance the high-frequency noise present in most natural images. To reduce the influence of noise, the most common solution is to smooth the input image before computing the derivative. The amount of smoothing can be selected based on the application or from prior knowledge of the amount of noise present in the input.

The solution that minimizes the influence of noise is to smooth the image with a Gaussian. In fact, as we just showed in the previous section, the Gaussian operator is the smoothing operator that best approximates the human visual system. If the Gaussian width is correctly selected, then this is the best solution. Unfortunately, the width of the Gaussian cannot always be determined. This is the case when we are working with 3D scenes, as objects at different depths will show different amounts of smoothing. Therefore, it is common to use a simpler smoothing operator that approximates the Gaussian but requires fewer operations. This approximation is obtained using the **binomial filter**.

The binomial filter has two main properties that make it an ideal candidate for smoothing images in order to compute the image derivatives:

- **It approximates a Gaussian:** It can be shown that if we increase the filter size, the binomial kernel approximates a Gaussian.
- **It can be computed using recursive operations:** As we will show later, the binomial kernel can be applied by just adding and multiplying pixel values without the need to keep a large filter in memory.

As we did with the Gaussian, we will first compute the derivative of the binomial kernel and then convolve the input image with it.

The binomial kernel is defined as follows:
$$
\mathbf{b_1} = \left[1, 1\right]/2
$$
Higher-order binomial filters can be computed recursively by convolving the binomial kernel with itself:
$$
\mathbf{b_2} = \mathbf{b_1} \circ \mathbf{b_1} = \left[1, 2, 1\right]/4
$$
$$
\mathbf{b_3} = \mathbf{b_2} \circ \mathbf{b_1} = \left[1, 3, 3, 1\right]/8
$$
$$
\mathbf{b_4} = \mathbf{b_3} \circ \mathbf{b_1} = \left[1, 4, 6, 4, 1\right]/16
$$
$$
\vdots
$$

In general, the $N$-th binomial filter, $\mathbf{b_N}$ is:
$$
b_N[n] = \frac{1}{2^N}
\begin{pmatrix}
N\\
n
\end{pmatrix}
$$
@fig-binomialkernels shows the binomial kernels for $N=1$ to $N=5$.

![Binomial kernels of increasing size.](figures/derivatives/binomial_kernels.png){#fig-binomialkernels}

The $N$-th binomial filter can be used to compute image derivatives by first convolving the input image with $\mathbf{b_N}$ and then applying a derivative operator $\mathbf{d}=\left[1,-1\right]$.

But note that this can be performed in one single step. If we first compute the derivative of the kernel and then convolve the image with it, the result is the same, and the cost is lower. The $N$-th binomial derivative operator is computed by convolving the binomial filter with $\mathbf{d}$:

$$
\mathbf{d_1} \circ \mathbf{b_1} = \mathbf{d_2} =  \left[1, 0, -1 \right]/2
$$
$$
\mathbf{d_2} \circ \mathbf{b_1} = \mathbf{d_3} =  \left[1, -2, 1 \right]/4
$$
$$
\mathbf{d_3} \circ \mathbf{b_1} = \mathbf{d_4} =  \left[1, -3, 3, -1 \right]/8
$$
$$
\vdots
$$
In general, the $N$-th derivative binomial filter is:
$$
d_N[n] = \frac{1}{2^N} \left( \binom{N}{n} - \binom{N}{n-1} \right)
$$
@fig-binomialderivatives shows the first derivatives of the binomial kernel for $N=2$ to $N=5$. This kernel can be used to approximate the derivative in a single step.

![Binomial derivative kernels of increasing size.](figures/derivatives/binomial_derivatives.png){#fig-binomialderivatives}

This method reduces noise and does not misalign the output. Another important advantage of binomial filters is that the convolution can be implemented using a recursive filter.

## Summary

In this chapter, we have reviewed how to compute image derivatives and the importance of filtering the input image before performing the derivative operation to reduce the influence of noise. The filters that we presented are linear and shift-invariant, and their implementation can be performed using convolution. We studied how binomial filters approximate a Gaussian and how their derivatives approximate the Gaussian derivative. Using binomial filters allows for reducing the influence of noise in a very efficient way, both in the spatial and frequency domains.
.